{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JuGyidmGfYJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0388118-deb1-4055-f281-d5084bac99c4"
      },
      "source": [
        "import string\n",
        "import re\n",
        "from pickle import dump\n",
        "from unicodedata import normalize\n",
        "from numpy import array\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, mode='rt', encoding='utf-8')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# split a loaded document into sentences\n",
        "def to_pairs(doc):\n",
        "\tlines = doc.strip().split('\\n')\n",
        "\tpairs = [line.split('\\t') for line in  lines]\n",
        "\treturn pairs\n",
        "\n",
        "# clean a list of lines\n",
        "def clean_pairs(lines):\n",
        "\tcleaned = list()\n",
        "\t# prepare regex for char filtering\n",
        "\tre_print = re.compile('[^%s]' % re.escape(string.printable))\n",
        "\t# prepare translation table for removing punctuation\n",
        "\ttable = str.maketrans('', '', string.punctuation)\n",
        "\tfor pair in lines:\n",
        "\t\tclean_pair = list()\n",
        "\t\tfor line in pair:\n",
        "\t\t\t# normalize unicode characters\n",
        "\t\t\tline = normalize('NFD', line).encode('ascii', 'ignore')\n",
        "\t\t\tline = line.decode('UTF-8')\n",
        "\t\t\t# tokenize on white space\n",
        "\t\t\tline = line.split()\n",
        "\t\t\t# convert to lowercase\n",
        "\t\t\tline = [word.lower() for word in line]\n",
        "\t\t\t# remove punctuation from each token\n",
        "\t\t\tline = [word.translate(table) for word in line]\n",
        "\t\t\t# remove non-printable chars form each token\n",
        "\t\t\tline = [re_print.sub('', w) for w in line]\n",
        "\t\t\t# remove tokens with numbers in them\n",
        "\t\t\tline = [word for word in line if word.isalpha()]\n",
        "\t\t\t# store as string\n",
        "\t\t\tclean_pair.append(' '.join(line))\n",
        "\t\tcleaned.append(clean_pair)\n",
        "\treturn array(cleaned)\n",
        "\n",
        "# save a list of clean sentences to file\n",
        "def save_clean_data(sentences, filename):\n",
        "\tdump(sentences, open(filename, 'wb'))\n",
        "\tprint('Saved: %s' % filename)\n",
        "\n",
        "# load dataset\n",
        "filename = 'dan.txt'\n",
        "doc = load_doc(filename)\n",
        "# split into english-german pairs\n",
        "pairs = to_pairs(doc)\n",
        "# clean sentences\n",
        "clean_pairs = clean_pairs(pairs)\n",
        "# save clean pairs to file\n",
        "save_clean_data(clean_pairs, 'english-danish.pkl')\n",
        "# spot check\n",
        "for i in range(100):\n",
        "\tprint('[%s] => [%s]' % (clean_pairs[i,0], clean_pairs[i,1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved: english-danish.pkl\n",
            "[go] => [ga]\n",
            "[hi] => [hej]\n",
            "[hi] => [hej]\n",
            "[run] => [lb]\n",
            "[run] => [lb]\n",
            "[wow] => [wow]\n",
            "[fire] => [fyr]\n",
            "[fire] => [ildebrand]\n",
            "[help] => [hjlp]\n",
            "[jump] => [hop]\n",
            "[jump] => [spring]\n",
            "[jump] => [hop]\n",
            "[jump] => [spring]\n",
            "[stop] => [stop]\n",
            "[hello] => [hej]\n",
            "[hello] => [hej]\n",
            "[i see] => [jeg forstar]\n",
            "[i won] => [jeg vandt]\n",
            "[shoot] => [skyd]\n",
            "[attack] => [angrib]\n",
            "[cheers] => [skal]\n",
            "[go now] => [ga nu]\n",
            "[he ran] => [han lb]\n",
            "[hop in] => [hop ind]\n",
            "[i lied] => [jeg lj]\n",
            "[i lied] => [jeg har ljet]\n",
            "[i sang] => [jeg sang]\n",
            "[no way] => [aldrig i livet]\n",
            "[no way] => [ikke tale om]\n",
            "[no way] => [absolut ikke]\n",
            "[no way] => [under ingen omstndigheder]\n",
            "[no way] => [aldrig i verden]\n",
            "[no way] => [ikke pa vilkar]\n",
            "[no way] => [du kan tro nej]\n",
            "[no way] => [du kan bande pa nej]\n",
            "[no way] => [niksen biksen]\n",
            "[no way] => [nul putte]\n",
            "[no way] => [det kan du glemme alt om]\n",
            "[no way] => [pa ingen made]\n",
            "[no way] => [det er ikke muligt]\n",
            "[really] => [virkelig]\n",
            "[really] => [er det rigtigt]\n",
            "[really] => [ja sa]\n",
            "[really] => [nej virkelig]\n",
            "[really] => [sa]\n",
            "[thanks] => [tak]\n",
            "[thanks] => [mange tak]\n",
            "[thanks] => [tak]\n",
            "[why me] => [hvorfor mig]\n",
            "[ask tom] => [sprg tom]\n",
            "[ask tom] => [sprg tom]\n",
            "[ask him] => [sprg ham]\n",
            "[be fair] => [vr retfrdig]\n",
            "[be fair] => [vr fair]\n",
            "[call me] => [ring til mig]\n",
            "[come in] => [kom ind]\n",
            "[get tom] => [hent tom]\n",
            "[go away] => [ga vk]\n",
            "[go away] => [ga din vej]\n",
            "[go away] => [forsvind med dig]\n",
            "[go away] => [forsvind]\n",
            "[help me] => [hjlp mig]\n",
            "[help me] => [hjlp mig]\n",
            "[i agree] => [jeg er enig]\n",
            "[im shy] => [jeg er genert]\n",
            "[its me] => [det er mig]\n",
            "[keep it] => [behold det]\n",
            "[keep it] => [behold den]\n",
            "[kiss me] => [kys mig]\n",
            "[perfect] => [perfekt]\n",
            "[see you] => [pa gensyn]\n",
            "[see you] => [vi ses]\n",
            "[see you] => [vi ses]\n",
            "[see you] => [vi ses]\n",
            "[show me] => [vis mig]\n",
            "[shut up] => [hold mund]\n",
            "[shut up] => [ti stille]\n",
            "[shut up] => [klap i]\n",
            "[shut up] => [hold kft]\n",
            "[we lost] => [vi tabte]\n",
            "[we lost] => [vi har tabt]\n",
            "[welcome] => [velkommen]\n",
            "[welcome] => [velkommen]\n",
            "[you won] => [du vandt]\n",
            "[you won] => [du har vundet]\n",
            "[be quiet] => [vr stille]\n",
            "[be quiet] => [ti stille]\n",
            "[be still] => [vr stille]\n",
            "[be still] => [vr stille]\n",
            "[be still] => [vr rolig]\n",
            "[catch me] => [grib mig]\n",
            "[catch me] => [fang mig]\n",
            "[cheer up] => [op med humret]\n",
            "[drive on] => [kr videre]\n",
            "[get away] => [ga vk]\n",
            "[get lost] => [ga vk]\n",
            "[get lost] => [smut]\n",
            "[get lost] => [ga din vej]\n",
            "[get lost] => [skrid]\n",
            "[get lost] => [skrub af]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6H4IZfnVb4o",
        "outputId": "6215fdb8-cd3b-47bb-b812-490b9b08df6f"
      },
      "source": [
        "import string\n",
        "import re\n",
        "from pickle import dump\n",
        "from unicodedata import normalize\n",
        "from numpy import array\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, mode='rt', encoding='utf-8')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# split a loaded document into sentences\n",
        "def to_pairs(doc):\n",
        "\tlines = doc.strip().split('\\n')\n",
        "\tpairs = [line.split('\\t') for line in  lines]\n",
        "\treturn pairs\n",
        "\n",
        "# clean a list of lines\n",
        "def clean_pairs(lines):\n",
        "\tcleaned = list()\n",
        "\t# prepare regex for char filtering\n",
        "\tre_print = re.compile('[^%s]' % re.escape(string.printable))\n",
        "\t# prepare translation table for removing punctuation\n",
        "\ttable = str.maketrans('', '', string.punctuation)\n",
        "\tfor pair in lines:\n",
        "\t\tclean_pair = list()\n",
        "\t\tfor line in pair:\n",
        "\t\t\t# normalize unicode characters\n",
        "\t\t\tline = normalize('NFD', line).encode('ascii', 'ignore')\n",
        "\t\t\tline = line.decode('UTF-8')\n",
        "\t\t\t# tokenize on white space\n",
        "\t\t\tline = line.split()\n",
        "\t\t\t# convert to lowercase\n",
        "\t\t\tline = [word.lower() for word in line]\n",
        "\t\t\t# remove punctuation from each token\n",
        "\t\t\tline = [word.translate(table) for word in line]\n",
        "\t\t\t# remove non-printable chars form each token\n",
        "\t\t\tline = [re_print.sub('', w) for w in line]\n",
        "\t\t\t# remove tokens with numbers in them\n",
        "\t\t\tline = [word for word in line if word.isalpha()]\n",
        "\t\t\t# store as string\n",
        "\t\t\tclean_pair.append(' '.join(line))\n",
        "\t\tcleaned.append(clean_pair)\n",
        "\treturn array(cleaned)\n",
        "\n",
        "# save a list of clean sentences to file\n",
        "def save_clean_data(sentences, filename):\n",
        "\tdump(sentences, open(filename, 'wb'))\n",
        "\tprint('Saved: %s' % filename)\n",
        "\n",
        "# load dataset\n",
        "filename = 'dan.txt'\n",
        "doc = load_doc(filename)\n",
        "# split into english-german pairs\n",
        "pairs = to_pairs(doc)\n",
        "# clean sentences\n",
        "clean_pairs = clean_pairs(pairs)\n",
        "# save clean pairs to file\n",
        "save_clean_data(clean_pairs, 'english-danish.pkl')\n",
        "# spot check\n",
        "for i in range(100):\n",
        "\tprint('[%s] => [%s]' % (clean_pairs[i,0], clean_pairs[i,1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved: english-danish.pkl\n",
            "[go] => [ga]\n",
            "[hi] => [hej]\n",
            "[hi] => [hej]\n",
            "[run] => [lb]\n",
            "[run] => [lb]\n",
            "[wow] => [wow]\n",
            "[fire] => [fyr]\n",
            "[fire] => [ildebrand]\n",
            "[help] => [hjlp]\n",
            "[jump] => [hop]\n",
            "[jump] => [spring]\n",
            "[jump] => [hop]\n",
            "[jump] => [spring]\n",
            "[stop] => [stop]\n",
            "[hello] => [hej]\n",
            "[hello] => [hej]\n",
            "[i see] => [jeg forstar]\n",
            "[i won] => [jeg vandt]\n",
            "[shoot] => [skyd]\n",
            "[attack] => [angrib]\n",
            "[cheers] => [skal]\n",
            "[go now] => [ga nu]\n",
            "[he ran] => [han lb]\n",
            "[hop in] => [hop ind]\n",
            "[i lied] => [jeg lj]\n",
            "[i lied] => [jeg har ljet]\n",
            "[i sang] => [jeg sang]\n",
            "[no way] => [aldrig i livet]\n",
            "[no way] => [ikke tale om]\n",
            "[no way] => [absolut ikke]\n",
            "[no way] => [under ingen omstndigheder]\n",
            "[no way] => [aldrig i verden]\n",
            "[no way] => [ikke pa vilkar]\n",
            "[no way] => [du kan tro nej]\n",
            "[no way] => [du kan bande pa nej]\n",
            "[no way] => [niksen biksen]\n",
            "[no way] => [nul putte]\n",
            "[no way] => [det kan du glemme alt om]\n",
            "[no way] => [pa ingen made]\n",
            "[no way] => [det er ikke muligt]\n",
            "[really] => [virkelig]\n",
            "[really] => [er det rigtigt]\n",
            "[really] => [ja sa]\n",
            "[really] => [nej virkelig]\n",
            "[really] => [sa]\n",
            "[thanks] => [tak]\n",
            "[thanks] => [mange tak]\n",
            "[thanks] => [tak]\n",
            "[why me] => [hvorfor mig]\n",
            "[ask tom] => [sprg tom]\n",
            "[ask tom] => [sprg tom]\n",
            "[ask him] => [sprg ham]\n",
            "[be fair] => [vr retfrdig]\n",
            "[be fair] => [vr fair]\n",
            "[call me] => [ring til mig]\n",
            "[come in] => [kom ind]\n",
            "[get tom] => [hent tom]\n",
            "[go away] => [ga vk]\n",
            "[go away] => [ga din vej]\n",
            "[go away] => [forsvind med dig]\n",
            "[go away] => [forsvind]\n",
            "[help me] => [hjlp mig]\n",
            "[help me] => [hjlp mig]\n",
            "[i agree] => [jeg er enig]\n",
            "[im shy] => [jeg er genert]\n",
            "[its me] => [det er mig]\n",
            "[keep it] => [behold det]\n",
            "[keep it] => [behold den]\n",
            "[kiss me] => [kys mig]\n",
            "[perfect] => [perfekt]\n",
            "[see you] => [pa gensyn]\n",
            "[see you] => [vi ses]\n",
            "[see you] => [vi ses]\n",
            "[see you] => [vi ses]\n",
            "[show me] => [vis mig]\n",
            "[shut up] => [hold mund]\n",
            "[shut up] => [ti stille]\n",
            "[shut up] => [klap i]\n",
            "[shut up] => [hold kft]\n",
            "[we lost] => [vi tabte]\n",
            "[we lost] => [vi har tabt]\n",
            "[welcome] => [velkommen]\n",
            "[welcome] => [velkommen]\n",
            "[you won] => [du vandt]\n",
            "[you won] => [du har vundet]\n",
            "[be quiet] => [vr stille]\n",
            "[be quiet] => [ti stille]\n",
            "[be still] => [vr stille]\n",
            "[be still] => [vr stille]\n",
            "[be still] => [vr rolig]\n",
            "[catch me] => [grib mig]\n",
            "[catch me] => [fang mig]\n",
            "[cheer up] => [op med humret]\n",
            "[drive on] => [kr videre]\n",
            "[get away] => [ga vk]\n",
            "[get lost] => [ga vk]\n",
            "[get lost] => [smut]\n",
            "[get lost] => [ga din vej]\n",
            "[get lost] => [skrid]\n",
            "[get lost] => [skrub af]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbFxwHatD0kg",
        "outputId": "47d8729e-ec9b-4d96-f0c4-ef19f7959a97"
      },
      "source": [
        "from pickle import load\n",
        "from pickle import dump\n",
        "from numpy.random import rand\n",
        "from numpy.random import shuffle\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# save a list of clean sentences to file\n",
        "def save_clean_data(sentences, filename):\n",
        "\tdump(sentences, open(filename, 'wb'))\n",
        "\tprint('Saved: %s' % filename)\n",
        "\n",
        "# load dataset\n",
        "raw_dataset = load_clean_sentences('english-danish.pkl')\n",
        "\n",
        "# reduce dataset size\n",
        "n_sentences = 10000\n",
        "dataset = raw_dataset[:n_sentences, :]\n",
        "# random shuffle\n",
        "shuffle(dataset)\n",
        "# split into train/test\n",
        "train, test = dataset[:9000], dataset[1000:]\n",
        "# save\n",
        "save_clean_data(dataset, 'english-danish-both.pkl')\n",
        "save_clean_data(train, 'english-danish-train.pkl')\n",
        "save_clean_data(test, 'english-danish-test.pkl')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved: english-danish-both.pkl\n",
            "Saved: english-danish-train.pkl\n",
            "Saved: english-danish-test.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qpd_iQJ0EgyG",
        "outputId": "6e02fd97-e8a0-4841-e309-ded056eb89d1"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# one hot encode target sequence\n",
        "def encode_output(sequences, vocab_size):\n",
        "\tylist = list()\n",
        "\tfor sequence in sequences:\n",
        "\t\tencoded = to_categorical(sequence, num_classes=vocab_size)\n",
        "\t\tylist.append(encoded)\n",
        "\ty = array(ylist)\n",
        "\ty = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
        "\treturn y\n",
        "\n",
        "# define NMT model\n",
        "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
        "\tmodel.add(LSTM(n_units))\n",
        "\tmodel.add(RepeatVector(tar_timesteps))\n",
        "\tmodel.add(LSTM(n_units, return_sequences=True))\n",
        "\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
        "\treturn model\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-danish-both.pkl')\n",
        "train = load_clean_sentences('english-danish-train.pkl')\n",
        "test = load_clean_sentences('english-danish-test.pkl')\n",
        "\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
        "print('English Max Length: %d' % (eng_length))\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "print('Danish Vocabulary Size: %d' % ger_vocab_size)\n",
        "print('Danish Max Length: %d' % (ger_length))\n",
        "\n",
        "# prepare training data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
        "trainY = encode_output(trainY, eng_vocab_size)\n",
        "# prepare validation data\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
        "testY = encode_output(testY, eng_vocab_size)\n",
        "\n",
        "# define model\n",
        "model = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 256)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "# summarize defined model\n",
        "print(model.summary())\n",
        "plot_model(model, to_file='model.png', show_shapes=True)\n",
        "# fit model\n",
        "filename = 'model.h5'\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "model.fit(trainX, trainY, epochs=100, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English Vocabulary Size: 3297\n",
            "English Max Length: 8\n",
            "Danish Vocabulary Size: 4246\n",
            "Danish Max Length: 11\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 11, 256)           1086976   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 256)               525312    \n",
            "_________________________________________________________________\n",
            "repeat_vector (RepeatVector) (None, 8, 256)            0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 8, 256)            525312    \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, 8, 3297)           847329    \n",
            "=================================================================\n",
            "Total params: 2,984,929\n",
            "Trainable params: 2,984,929\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/30\n",
            "141/141 - 52s - loss: 4.0724 - val_loss: 3.2909\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 3.29086, saving model to model.h5\n",
            "Epoch 2/30\n",
            "141/141 - 44s - loss: 3.2024 - val_loss: 3.0932\n",
            "\n",
            "Epoch 00002: val_loss improved from 3.29086 to 3.09324, saving model to model.h5\n",
            "Epoch 3/30\n",
            "141/141 - 48s - loss: 3.0792 - val_loss: 3.0067\n",
            "\n",
            "Epoch 00003: val_loss improved from 3.09324 to 3.00674, saving model to model.h5\n",
            "Epoch 4/30\n",
            "141/141 - 44s - loss: 3.0026 - val_loss: 2.9416\n",
            "\n",
            "Epoch 00004: val_loss improved from 3.00674 to 2.94162, saving model to model.h5\n",
            "Epoch 5/30\n",
            "141/141 - 44s - loss: 2.9411 - val_loss: 2.8870\n",
            "\n",
            "Epoch 00005: val_loss improved from 2.94162 to 2.88703, saving model to model.h5\n",
            "Epoch 6/30\n",
            "141/141 - 43s - loss: 2.8643 - val_loss: 2.7873\n",
            "\n",
            "Epoch 00006: val_loss improved from 2.88703 to 2.78732, saving model to model.h5\n",
            "Epoch 7/30\n",
            "141/141 - 43s - loss: 2.7668 - val_loss: 2.6923\n",
            "\n",
            "Epoch 00007: val_loss improved from 2.78732 to 2.69226, saving model to model.h5\n",
            "Epoch 8/30\n",
            "141/141 - 44s - loss: 2.6699 - val_loss: 2.6026\n",
            "\n",
            "Epoch 00008: val_loss improved from 2.69226 to 2.60263, saving model to model.h5\n",
            "Epoch 9/30\n",
            "141/141 - 44s - loss: 2.5698 - val_loss: 2.4964\n",
            "\n",
            "Epoch 00009: val_loss improved from 2.60263 to 2.49636, saving model to model.h5\n",
            "Epoch 10/30\n",
            "141/141 - 44s - loss: 2.4522 - val_loss: 2.3662\n",
            "\n",
            "Epoch 00010: val_loss improved from 2.49636 to 2.36621, saving model to model.h5\n",
            "Epoch 11/30\n",
            "141/141 - 45s - loss: 2.3215 - val_loss: 2.2425\n",
            "\n",
            "Epoch 00011: val_loss improved from 2.36621 to 2.24250, saving model to model.h5\n",
            "Epoch 12/30\n",
            "141/141 - 44s - loss: 2.2008 - val_loss: 2.1262\n",
            "\n",
            "Epoch 00012: val_loss improved from 2.24250 to 2.12618, saving model to model.h5\n",
            "Epoch 13/30\n",
            "141/141 - 44s - loss: 2.0802 - val_loss: 2.0148\n",
            "\n",
            "Epoch 00013: val_loss improved from 2.12618 to 2.01480, saving model to model.h5\n",
            "Epoch 14/30\n",
            "141/141 - 44s - loss: 1.9660 - val_loss: 1.9049\n",
            "\n",
            "Epoch 00014: val_loss improved from 2.01480 to 1.90492, saving model to model.h5\n",
            "Epoch 15/30\n",
            "141/141 - 45s - loss: 1.8550 - val_loss: 1.7997\n",
            "\n",
            "Epoch 00015: val_loss improved from 1.90492 to 1.79972, saving model to model.h5\n",
            "Epoch 16/30\n",
            "141/141 - 44s - loss: 1.7502 - val_loss: 1.7077\n",
            "\n",
            "Epoch 00016: val_loss improved from 1.79972 to 1.70766, saving model to model.h5\n",
            "Epoch 17/30\n",
            "141/141 - 44s - loss: 1.6489 - val_loss: 1.6180\n",
            "\n",
            "Epoch 00017: val_loss improved from 1.70766 to 1.61805, saving model to model.h5\n",
            "Epoch 18/30\n",
            "141/141 - 44s - loss: 1.5507 - val_loss: 1.5186\n",
            "\n",
            "Epoch 00018: val_loss improved from 1.61805 to 1.51857, saving model to model.h5\n",
            "Epoch 19/30\n",
            "141/141 - 43s - loss: 1.4535 - val_loss: 1.4391\n",
            "\n",
            "Epoch 00019: val_loss improved from 1.51857 to 1.43905, saving model to model.h5\n",
            "Epoch 20/30\n",
            "141/141 - 44s - loss: 1.3627 - val_loss: 1.3499\n",
            "\n",
            "Epoch 00020: val_loss improved from 1.43905 to 1.34987, saving model to model.h5\n",
            "Epoch 21/30\n",
            "141/141 - 44s - loss: 1.2741 - val_loss: 1.2740\n",
            "\n",
            "Epoch 00021: val_loss improved from 1.34987 to 1.27402, saving model to model.h5\n",
            "Epoch 22/30\n",
            "141/141 - 44s - loss: 1.1903 - val_loss: 1.1967\n",
            "\n",
            "Epoch 00022: val_loss improved from 1.27402 to 1.19674, saving model to model.h5\n",
            "Epoch 23/30\n",
            "141/141 - 44s - loss: 1.1121 - val_loss: 1.1253\n",
            "\n",
            "Epoch 00023: val_loss improved from 1.19674 to 1.12534, saving model to model.h5\n",
            "Epoch 24/30\n",
            "141/141 - 44s - loss: 1.0325 - val_loss: 1.0630\n",
            "\n",
            "Epoch 00024: val_loss improved from 1.12534 to 1.06303, saving model to model.h5\n",
            "Epoch 25/30\n",
            "141/141 - 46s - loss: 0.9605 - val_loss: 0.9941\n",
            "\n",
            "Epoch 00025: val_loss improved from 1.06303 to 0.99414, saving model to model.h5\n",
            "Epoch 26/30\n",
            "141/141 - 45s - loss: 0.8929 - val_loss: 0.9375\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.99414 to 0.93753, saving model to model.h5\n",
            "Epoch 27/30\n",
            "141/141 - 45s - loss: 0.8338 - val_loss: 0.8868\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.93753 to 0.88683, saving model to model.h5\n",
            "Epoch 28/30\n",
            "141/141 - 45s - loss: 0.7713 - val_loss: 0.8374\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.88683 to 0.83735, saving model to model.h5\n",
            "Epoch 29/30\n",
            "141/141 - 45s - loss: 0.7122 - val_loss: 0.7905\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.83735 to 0.79052, saving model to model.h5\n",
            "Epoch 30/30\n",
            "141/141 - 45s - loss: 0.6584 - val_loss: 0.7406\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.79052 to 0.74065, saving model to model.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f252e1a3a58>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0CZ5If8hWyv",
        "outputId": "b57312e1-4b13-4397-f41a-5ca90dc79aaa"
      },
      "source": [
        "from pickle import load\n",
        "from pickle import dump\n",
        "from numpy.random import rand\n",
        "from numpy.random import shuffle\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# save a list of clean sentences to file\n",
        "def save_clean_data(sentences, filename):\n",
        "\tdump(sentences, open(filename, 'wb'))\n",
        "\tprint('Saved: %s' % filename)\n",
        "\n",
        "# load dataset\n",
        "raw_dataset = load_clean_sentences('english-danish.pkl')\n",
        "\n",
        "# reduce dataset size\n",
        "n_sentences = 10000\n",
        "dataset = raw_dataset[:n_sentences, :]\n",
        "# random shuffle\n",
        "shuffle(dataset)\n",
        "# split into train/test\n",
        "train, test = dataset[:9000], dataset[1000:]\n",
        "# save\n",
        "save_clean_data(dataset, 'english-danish-both.pkl')\n",
        "save_clean_data(train, 'english-danish-train.pkl')\n",
        "save_clean_data(test, 'english-danish-test.pkl')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved: english-danish-both.pkl\n",
            "Saved: english-danish-train.pkl\n",
            "Saved: english-danish-test.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgmHAPu4hXkc",
        "outputId": "5e6399af-ba08-4416-a8fc-37b0fb904eb0"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# one hot encode target sequence\n",
        "def encode_output(sequences, vocab_size):\n",
        "\tylist = list()\n",
        "\tfor sequence in sequences:\n",
        "\t\tencoded = to_categorical(sequence, num_classes=vocab_size)\n",
        "\t\tylist.append(encoded)\n",
        "\ty = array(ylist)\n",
        "\ty = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
        "\treturn y\n",
        "\n",
        "# define NMT model\n",
        "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
        "\tmodel.add(LSTM(n_units))\n",
        "\tmodel.add(RepeatVector(tar_timesteps))\n",
        "\tmodel.add(LSTM(n_units, return_sequences=True))\n",
        "\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
        "\treturn model\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-danish-both.pkl')\n",
        "train = load_clean_sentences('english-danish-train.pkl')\n",
        "test = load_clean_sentences('english-danish-test.pkl')\n",
        "\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
        "print('English Max Length: %d' % (eng_length))\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "print('Danish Vocabulary Size: %d' % ger_vocab_size)\n",
        "print('Danish Max Length: %d' % (ger_length))\n",
        "\n",
        "# prepare training data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
        "trainY = encode_output(trainY, eng_vocab_size)\n",
        "# prepare validation data\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
        "testY = encode_output(testY, eng_vocab_size)\n",
        "\n",
        "# define model\n",
        "model = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 256)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "# summarize defined model\n",
        "print(model.summary())\n",
        "plot_model(model, to_file='model.png', show_shapes=True)\n",
        "# fit model\n",
        "filename = 'model.h5'\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "model.fit(trainX, trainY, epochs=100, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English Vocabulary Size: 3297\n",
            "English Max Length: 8\n",
            "Danish Vocabulary Size: 4246\n",
            "Danish Max Length: 11\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 11, 256)           1086976   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 256)               525312    \n",
            "_________________________________________________________________\n",
            "repeat_vector (RepeatVector) (None, 8, 256)            0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 8, 256)            525312    \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, 8, 3297)           847329    \n",
            "=================================================================\n",
            "Total params: 2,984,929\n",
            "Trainable params: 2,984,929\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "141/141 - 41s - loss: 4.0657 - val_loss: 3.3017\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 3.30165, saving model to model.h5\n",
            "Epoch 2/100\n",
            "141/141 - 35s - loss: 3.2105 - val_loss: 3.1205\n",
            "\n",
            "Epoch 00002: val_loss improved from 3.30165 to 3.12049, saving model to model.h5\n",
            "Epoch 3/100\n",
            "141/141 - 35s - loss: 3.1021 - val_loss: 3.0456\n",
            "\n",
            "Epoch 00003: val_loss improved from 3.12049 to 3.04563, saving model to model.h5\n",
            "Epoch 4/100\n",
            "141/141 - 35s - loss: 3.0402 - val_loss: 2.9902\n",
            "\n",
            "Epoch 00004: val_loss improved from 3.04563 to 2.99021, saving model to model.h5\n",
            "Epoch 5/100\n",
            "141/141 - 35s - loss: 2.9755 - val_loss: 2.9261\n",
            "\n",
            "Epoch 00005: val_loss improved from 2.99021 to 2.92606, saving model to model.h5\n",
            "Epoch 6/100\n",
            "141/141 - 35s - loss: 2.9041 - val_loss: 2.8346\n",
            "\n",
            "Epoch 00006: val_loss improved from 2.92606 to 2.83461, saving model to model.h5\n",
            "Epoch 7/100\n",
            "141/141 - 35s - loss: 2.7994 - val_loss: 2.7426\n",
            "\n",
            "Epoch 00007: val_loss improved from 2.83461 to 2.74262, saving model to model.h5\n",
            "Epoch 8/100\n",
            "141/141 - 35s - loss: 2.7268 - val_loss: 2.6879\n",
            "\n",
            "Epoch 00008: val_loss improved from 2.74262 to 2.68786, saving model to model.h5\n",
            "Epoch 9/100\n",
            "141/141 - 35s - loss: 2.6562 - val_loss: 2.6074\n",
            "\n",
            "Epoch 00009: val_loss improved from 2.68786 to 2.60740, saving model to model.h5\n",
            "Epoch 10/100\n",
            "141/141 - 35s - loss: 2.5683 - val_loss: 2.5062\n",
            "\n",
            "Epoch 00010: val_loss improved from 2.60740 to 2.50623, saving model to model.h5\n",
            "Epoch 11/100\n",
            "141/141 - 36s - loss: 2.4614 - val_loss: 2.4015\n",
            "\n",
            "Epoch 00011: val_loss improved from 2.50623 to 2.40154, saving model to model.h5\n",
            "Epoch 12/100\n",
            "141/141 - 37s - loss: 2.3541 - val_loss: 2.3052\n",
            "\n",
            "Epoch 00012: val_loss improved from 2.40154 to 2.30522, saving model to model.h5\n",
            "Epoch 13/100\n",
            "141/141 - 36s - loss: 2.2509 - val_loss: 2.1979\n",
            "\n",
            "Epoch 00013: val_loss improved from 2.30522 to 2.19789, saving model to model.h5\n",
            "Epoch 14/100\n",
            "141/141 - 35s - loss: 2.1468 - val_loss: 2.1107\n",
            "\n",
            "Epoch 00014: val_loss improved from 2.19789 to 2.11073, saving model to model.h5\n",
            "Epoch 15/100\n",
            "141/141 - 35s - loss: 2.0430 - val_loss: 1.9951\n",
            "\n",
            "Epoch 00015: val_loss improved from 2.11073 to 1.99515, saving model to model.h5\n",
            "Epoch 16/100\n",
            "141/141 - 35s - loss: 1.9382 - val_loss: 1.9031\n",
            "\n",
            "Epoch 00016: val_loss improved from 1.99515 to 1.90311, saving model to model.h5\n",
            "Epoch 17/100\n",
            "141/141 - 35s - loss: 1.8391 - val_loss: 1.8249\n",
            "\n",
            "Epoch 00017: val_loss improved from 1.90311 to 1.82493, saving model to model.h5\n",
            "Epoch 18/100\n",
            "141/141 - 35s - loss: 1.7421 - val_loss: 1.7142\n",
            "\n",
            "Epoch 00018: val_loss improved from 1.82493 to 1.71417, saving model to model.h5\n",
            "Epoch 19/100\n",
            "141/141 - 35s - loss: 1.6503 - val_loss: 1.6260\n",
            "\n",
            "Epoch 00019: val_loss improved from 1.71417 to 1.62595, saving model to model.h5\n",
            "Epoch 20/100\n",
            "141/141 - 35s - loss: 1.5591 - val_loss: 1.5455\n",
            "\n",
            "Epoch 00020: val_loss improved from 1.62595 to 1.54549, saving model to model.h5\n",
            "Epoch 21/100\n",
            "141/141 - 35s - loss: 1.4726 - val_loss: 1.4694\n",
            "\n",
            "Epoch 00021: val_loss improved from 1.54549 to 1.46937, saving model to model.h5\n",
            "Epoch 22/100\n",
            "141/141 - 35s - loss: 1.3874 - val_loss: 1.3881\n",
            "\n",
            "Epoch 00022: val_loss improved from 1.46937 to 1.38812, saving model to model.h5\n",
            "Epoch 23/100\n",
            "141/141 - 35s - loss: 1.3057 - val_loss: 1.3123\n",
            "\n",
            "Epoch 00023: val_loss improved from 1.38812 to 1.31225, saving model to model.h5\n",
            "Epoch 24/100\n",
            "141/141 - 35s - loss: 1.2311 - val_loss: 1.2456\n",
            "\n",
            "Epoch 00024: val_loss improved from 1.31225 to 1.24559, saving model to model.h5\n",
            "Epoch 25/100\n",
            "141/141 - 35s - loss: 1.1524 - val_loss: 1.1749\n",
            "\n",
            "Epoch 00025: val_loss improved from 1.24559 to 1.17491, saving model to model.h5\n",
            "Epoch 26/100\n",
            "141/141 - 37s - loss: 1.0785 - val_loss: 1.1083\n",
            "\n",
            "Epoch 00026: val_loss improved from 1.17491 to 1.10832, saving model to model.h5\n",
            "Epoch 27/100\n",
            "141/141 - 39s - loss: 1.0085 - val_loss: 1.0470\n",
            "\n",
            "Epoch 00027: val_loss improved from 1.10832 to 1.04697, saving model to model.h5\n",
            "Epoch 28/100\n",
            "141/141 - 37s - loss: 0.9447 - val_loss: 0.9936\n",
            "\n",
            "Epoch 00028: val_loss improved from 1.04697 to 0.99356, saving model to model.h5\n",
            "Epoch 29/100\n",
            "141/141 - 36s - loss: 0.8806 - val_loss: 0.9393\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.99356 to 0.93930, saving model to model.h5\n",
            "Epoch 30/100\n",
            "141/141 - 36s - loss: 0.8229 - val_loss: 0.8938\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.93930 to 0.89379, saving model to model.h5\n",
            "Epoch 31/100\n",
            "141/141 - 36s - loss: 0.7679 - val_loss: 0.8440\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.89379 to 0.84396, saving model to model.h5\n",
            "Epoch 32/100\n",
            "141/141 - 36s - loss: 0.7158 - val_loss: 0.7972\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.84396 to 0.79723, saving model to model.h5\n",
            "Epoch 33/100\n",
            "141/141 - 36s - loss: 0.6675 - val_loss: 0.7572\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.79723 to 0.75717, saving model to model.h5\n",
            "Epoch 34/100\n",
            "141/141 - 36s - loss: 0.6219 - val_loss: 0.7211\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.75717 to 0.72115, saving model to model.h5\n",
            "Epoch 35/100\n",
            "141/141 - 36s - loss: 0.5783 - val_loss: 0.6851\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.72115 to 0.68506, saving model to model.h5\n",
            "Epoch 36/100\n",
            "141/141 - 36s - loss: 0.5397 - val_loss: 0.6544\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.68506 to 0.65441, saving model to model.h5\n",
            "Epoch 37/100\n",
            "141/141 - 35s - loss: 0.5049 - val_loss: 0.6361\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.65441 to 0.63615, saving model to model.h5\n",
            "Epoch 38/100\n",
            "141/141 - 35s - loss: 0.4686 - val_loss: 0.5948\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.63615 to 0.59481, saving model to model.h5\n",
            "Epoch 39/100\n",
            "141/141 - 35s - loss: 0.4359 - val_loss: 0.5684\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.59481 to 0.56841, saving model to model.h5\n",
            "Epoch 40/100\n",
            "141/141 - 35s - loss: 0.4030 - val_loss: 0.5476\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.56841 to 0.54757, saving model to model.h5\n",
            "Epoch 41/100\n",
            "141/141 - 35s - loss: 0.3776 - val_loss: 0.5246\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.54757 to 0.52457, saving model to model.h5\n",
            "Epoch 42/100\n",
            "141/141 - 35s - loss: 0.3531 - val_loss: 0.5043\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.52457 to 0.50426, saving model to model.h5\n",
            "Epoch 43/100\n",
            "141/141 - 35s - loss: 0.3297 - val_loss: 0.4868\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.50426 to 0.48677, saving model to model.h5\n",
            "Epoch 44/100\n",
            "141/141 - 35s - loss: 0.3093 - val_loss: 0.4682\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.48677 to 0.46817, saving model to model.h5\n",
            "Epoch 45/100\n",
            "141/141 - 35s - loss: 0.2858 - val_loss: 0.4546\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.46817 to 0.45462, saving model to model.h5\n",
            "Epoch 46/100\n",
            "141/141 - 35s - loss: 0.2672 - val_loss: 0.4391\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.45462 to 0.43907, saving model to model.h5\n",
            "Epoch 47/100\n",
            "141/141 - 35s - loss: 0.2508 - val_loss: 0.4271\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.43907 to 0.42706, saving model to model.h5\n",
            "Epoch 48/100\n",
            "141/141 - 35s - loss: 0.2349 - val_loss: 0.4164\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.42706 to 0.41635, saving model to model.h5\n",
            "Epoch 49/100\n",
            "141/141 - 34s - loss: 0.2226 - val_loss: 0.4048\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.41635 to 0.40483, saving model to model.h5\n",
            "Epoch 50/100\n",
            "141/141 - 35s - loss: 0.2090 - val_loss: 0.3942\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.40483 to 0.39423, saving model to model.h5\n",
            "Epoch 51/100\n",
            "141/141 - 35s - loss: 0.1961 - val_loss: 0.3862\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.39423 to 0.38619, saving model to model.h5\n",
            "Epoch 52/100\n",
            "141/141 - 35s - loss: 0.1852 - val_loss: 0.3751\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.38619 to 0.37511, saving model to model.h5\n",
            "Epoch 53/100\n",
            "141/141 - 35s - loss: 0.1740 - val_loss: 0.3670\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.37511 to 0.36700, saving model to model.h5\n",
            "Epoch 54/100\n",
            "141/141 - 35s - loss: 0.1633 - val_loss: 0.3669\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.36700 to 0.36686, saving model to model.h5\n",
            "Epoch 55/100\n",
            "141/141 - 34s - loss: 0.1565 - val_loss: 0.3569\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.36686 to 0.35688, saving model to model.h5\n",
            "Epoch 56/100\n",
            "141/141 - 35s - loss: 0.1485 - val_loss: 0.3489\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.35688 to 0.34890, saving model to model.h5\n",
            "Epoch 57/100\n",
            "141/141 - 36s - loss: 0.1408 - val_loss: 0.3487\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.34890 to 0.34867, saving model to model.h5\n",
            "Epoch 58/100\n",
            "141/141 - 35s - loss: 0.1341 - val_loss: 0.3407\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.34867 to 0.34073, saving model to model.h5\n",
            "Epoch 59/100\n",
            "141/141 - 35s - loss: 0.1282 - val_loss: 0.3394\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.34073 to 0.33944, saving model to model.h5\n",
            "Epoch 60/100\n",
            "141/141 - 34s - loss: 0.1232 - val_loss: 0.3363\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.33944 to 0.33633, saving model to model.h5\n",
            "Epoch 61/100\n",
            "141/141 - 35s - loss: 0.1192 - val_loss: 0.3315\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.33633 to 0.33147, saving model to model.h5\n",
            "Epoch 62/100\n",
            "141/141 - 35s - loss: 0.1151 - val_loss: 0.3314\n",
            "\n",
            "Epoch 00062: val_loss improved from 0.33147 to 0.33144, saving model to model.h5\n",
            "Epoch 63/100\n",
            "141/141 - 35s - loss: 0.1100 - val_loss: 0.3272\n",
            "\n",
            "Epoch 00063: val_loss improved from 0.33144 to 0.32720, saving model to model.h5\n",
            "Epoch 64/100\n",
            "141/141 - 35s - loss: 0.1052 - val_loss: 0.3254\n",
            "\n",
            "Epoch 00064: val_loss improved from 0.32720 to 0.32544, saving model to model.h5\n",
            "Epoch 65/100\n",
            "141/141 - 35s - loss: 0.1011 - val_loss: 0.3215\n",
            "\n",
            "Epoch 00065: val_loss improved from 0.32544 to 0.32151, saving model to model.h5\n",
            "Epoch 66/100\n",
            "141/141 - 35s - loss: 0.0985 - val_loss: 0.3188\n",
            "\n",
            "Epoch 00066: val_loss improved from 0.32151 to 0.31876, saving model to model.h5\n",
            "Epoch 67/100\n",
            "141/141 - 35s - loss: 0.0952 - val_loss: 0.3180\n",
            "\n",
            "Epoch 00067: val_loss improved from 0.31876 to 0.31802, saving model to model.h5\n",
            "Epoch 68/100\n",
            "141/141 - 35s - loss: 0.0937 - val_loss: 0.3202\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.31802\n",
            "Epoch 69/100\n",
            "141/141 - 34s - loss: 0.0915 - val_loss: 0.3154\n",
            "\n",
            "Epoch 00069: val_loss improved from 0.31802 to 0.31536, saving model to model.h5\n",
            "Epoch 70/100\n",
            "141/141 - 35s - loss: 0.0908 - val_loss: 0.3167\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.31536\n",
            "Epoch 71/100\n",
            "141/141 - 35s - loss: 0.0882 - val_loss: 0.3164\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.31536\n",
            "Epoch 72/100\n",
            "141/141 - 35s - loss: 0.0869 - val_loss: 0.3174\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.31536\n",
            "Epoch 73/100\n",
            "141/141 - 35s - loss: 0.0878 - val_loss: 0.3159\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.31536\n",
            "Epoch 74/100\n",
            "141/141 - 35s - loss: 0.0854 - val_loss: 0.3131\n",
            "\n",
            "Epoch 00074: val_loss improved from 0.31536 to 0.31306, saving model to model.h5\n",
            "Epoch 75/100\n",
            "141/141 - 35s - loss: 0.0836 - val_loss: 0.3099\n",
            "\n",
            "Epoch 00075: val_loss improved from 0.31306 to 0.30989, saving model to model.h5\n",
            "Epoch 76/100\n",
            "141/141 - 35s - loss: 0.0795 - val_loss: 0.3136\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.30989\n",
            "Epoch 77/100\n",
            "141/141 - 34s - loss: 0.0786 - val_loss: 0.3108\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.30989\n",
            "Epoch 78/100\n",
            "141/141 - 35s - loss: 0.0774 - val_loss: 0.3099\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.30989\n",
            "Epoch 79/100\n",
            "141/141 - 35s - loss: 0.0751 - val_loss: 0.3110\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.30989\n",
            "Epoch 80/100\n",
            "141/141 - 35s - loss: 0.0749 - val_loss: 0.3094\n",
            "\n",
            "Epoch 00080: val_loss improved from 0.30989 to 0.30941, saving model to model.h5\n",
            "Epoch 81/100\n",
            "141/141 - 35s - loss: 0.0731 - val_loss: 0.3087\n",
            "\n",
            "Epoch 00081: val_loss improved from 0.30941 to 0.30870, saving model to model.h5\n",
            "Epoch 82/100\n",
            "141/141 - 35s - loss: 0.0719 - val_loss: 0.3086\n",
            "\n",
            "Epoch 00082: val_loss improved from 0.30870 to 0.30862, saving model to model.h5\n",
            "Epoch 83/100\n",
            "141/141 - 35s - loss: 0.0714 - val_loss: 0.3086\n",
            "\n",
            "Epoch 00083: val_loss improved from 0.30862 to 0.30856, saving model to model.h5\n",
            "Epoch 84/100\n",
            "141/141 - 35s - loss: 0.0713 - val_loss: 0.3100\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.30856\n",
            "Epoch 85/100\n",
            "141/141 - 35s - loss: 0.0723 - val_loss: 0.3100\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.30856\n",
            "Epoch 86/100\n",
            "141/141 - 35s - loss: 0.0709 - val_loss: 0.3099\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.30856\n",
            "Epoch 87/100\n",
            "141/141 - 35s - loss: 0.0703 - val_loss: 0.3089\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.30856\n",
            "Epoch 88/100\n",
            "141/141 - 35s - loss: 0.0694 - val_loss: 0.3108\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.30856\n",
            "Epoch 89/100\n",
            "141/141 - 35s - loss: 0.0761 - val_loss: 0.3136\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.30856\n",
            "Epoch 90/100\n",
            "141/141 - 35s - loss: 0.0771 - val_loss: 0.3170\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.30856\n",
            "Epoch 91/100\n",
            "141/141 - 35s - loss: 0.0760 - val_loss: 0.3163\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.30856\n",
            "Epoch 92/100\n",
            "141/141 - 35s - loss: 0.0740 - val_loss: 0.3117\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.30856\n",
            "Epoch 93/100\n",
            "141/141 - 35s - loss: 0.0704 - val_loss: 0.3132\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.30856\n",
            "Epoch 94/100\n",
            "141/141 - 35s - loss: 0.0679 - val_loss: 0.3092\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.30856\n",
            "Epoch 95/100\n",
            "141/141 - 35s - loss: 0.0660 - val_loss: 0.3095\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.30856\n",
            "Epoch 96/100\n",
            "141/141 - 35s - loss: 0.0648 - val_loss: 0.3115\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.30856\n",
            "Epoch 97/100\n",
            "141/141 - 35s - loss: 0.0635 - val_loss: 0.3100\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.30856\n",
            "Epoch 98/100\n",
            "141/141 - 35s - loss: 0.0636 - val_loss: 0.3105\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.30856\n",
            "Epoch 99/100\n",
            "141/141 - 35s - loss: 0.0629 - val_loss: 0.3085\n",
            "\n",
            "Epoch 00099: val_loss improved from 0.30856 to 0.30852, saving model to model.h5\n",
            "Epoch 100/100\n",
            "141/141 - 35s - loss: 0.0619 - val_loss: 0.3119\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.30852\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc79bb5c510>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyy-RKjiFLec",
        "outputId": "54e1b40d-934b-408c-bda0-2e524c2c458e"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# one hot encode target sequence\n",
        "def encode_output(sequences, vocab_size):\n",
        "\tylist = list()\n",
        "\tfor sequence in sequences:\n",
        "\t\tencoded = to_categorical(sequence, num_classes=vocab_size)\n",
        "\t\tylist.append(encoded)\n",
        "\ty = array(ylist)\n",
        "\ty = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
        "\treturn y\n",
        "\n",
        "# define NMT model\n",
        "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
        "\tmodel.add(LSTM(n_units))\n",
        "\tmodel.add(RepeatVector(tar_timesteps))\n",
        "\tmodel.add(LSTM(n_units, return_sequences=True))\n",
        "\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
        "\treturn model\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-danish-both.pkl')\n",
        "train = load_clean_sentences('english-danish-train.pkl')\n",
        "test = load_clean_sentences('english-danish-test.pkl')\n",
        "\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
        "print('English Max Length: %d' % (eng_length))\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "print('Danish Vocabulary Size: %d' % ger_vocab_size)\n",
        "print('Danish Max Length: %d' % (ger_length))\n",
        "\n",
        "# prepare training data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
        "trainY = encode_output(trainY, eng_vocab_size)\n",
        "# prepare validation data\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
        "testY = encode_output(testY, eng_vocab_size)\n",
        "\n",
        "# define model\n",
        "model = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 256)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "# summarize defined model\n",
        "print(model.summary())\n",
        "plot_model(model, to_file='model.png', show_shapes=True)\n",
        "# fit model\n",
        "filename = 'model.h5'\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "model.fit(trainX, trainY, epochs=200, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English Vocabulary Size: 3297\n",
            "English Max Length: 8\n",
            "Danish Vocabulary Size: 4246\n",
            "Danish Max Length: 11\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 11, 256)           1086976   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 256)               525312    \n",
            "_________________________________________________________________\n",
            "repeat_vector (RepeatVector) (None, 8, 256)            0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 8, 256)            525312    \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, 8, 3297)           847329    \n",
            "=================================================================\n",
            "Total params: 2,984,929\n",
            "Trainable params: 2,984,929\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "141/141 - 50s - loss: 4.0557 - val_loss: 3.2998\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 3.29982, saving model to model.h5\n",
            "Epoch 2/200\n",
            "141/141 - 42s - loss: 3.1955 - val_loss: 3.1103\n",
            "\n",
            "Epoch 00002: val_loss improved from 3.29982 to 3.11026, saving model to model.h5\n",
            "Epoch 3/200\n",
            "141/141 - 42s - loss: 3.0827 - val_loss: 3.0100\n",
            "\n",
            "Epoch 00003: val_loss improved from 3.11026 to 3.01003, saving model to model.h5\n",
            "Epoch 4/200\n",
            "141/141 - 46s - loss: 3.0005 - val_loss: 2.9424\n",
            "\n",
            "Epoch 00004: val_loss improved from 3.01003 to 2.94235, saving model to model.h5\n",
            "Epoch 5/200\n",
            "141/141 - 42s - loss: 2.9399 - val_loss: 2.8888\n",
            "\n",
            "Epoch 00005: val_loss improved from 2.94235 to 2.88880, saving model to model.h5\n",
            "Epoch 6/200\n",
            "141/141 - 43s - loss: 2.8831 - val_loss: 2.8266\n",
            "\n",
            "Epoch 00006: val_loss improved from 2.88880 to 2.82658, saving model to model.h5\n",
            "Epoch 7/200\n",
            "141/141 - 42s - loss: 2.7979 - val_loss: 2.7406\n",
            "\n",
            "Epoch 00007: val_loss improved from 2.82658 to 2.74057, saving model to model.h5\n",
            "Epoch 8/200\n",
            "141/141 - 43s - loss: 2.7107 - val_loss: 2.6462\n",
            "\n",
            "Epoch 00008: val_loss improved from 2.74057 to 2.64618, saving model to model.h5\n",
            "Epoch 9/200\n",
            "141/141 - 42s - loss: 2.6118 - val_loss: 2.5414\n",
            "\n",
            "Epoch 00009: val_loss improved from 2.64618 to 2.54136, saving model to model.h5\n",
            "Epoch 10/200\n",
            "141/141 - 42s - loss: 2.5022 - val_loss: 2.4393\n",
            "\n",
            "Epoch 00010: val_loss improved from 2.54136 to 2.43928, saving model to model.h5\n",
            "Epoch 11/200\n",
            "141/141 - 43s - loss: 2.3833 - val_loss: 2.3169\n",
            "\n",
            "Epoch 00011: val_loss improved from 2.43928 to 2.31687, saving model to model.h5\n",
            "Epoch 12/200\n",
            "141/141 - 43s - loss: 2.2682 - val_loss: 2.1978\n",
            "\n",
            "Epoch 00012: val_loss improved from 2.31687 to 2.19779, saving model to model.h5\n",
            "Epoch 13/200\n",
            "141/141 - 43s - loss: 2.1506 - val_loss: 2.0923\n",
            "\n",
            "Epoch 00013: val_loss improved from 2.19779 to 2.09230, saving model to model.h5\n",
            "Epoch 14/200\n",
            "141/141 - 42s - loss: 2.0372 - val_loss: 1.9810\n",
            "\n",
            "Epoch 00014: val_loss improved from 2.09230 to 1.98097, saving model to model.h5\n",
            "Epoch 15/200\n",
            "141/141 - 43s - loss: 1.9273 - val_loss: 1.8839\n",
            "\n",
            "Epoch 00015: val_loss improved from 1.98097 to 1.88388, saving model to model.h5\n",
            "Epoch 16/200\n",
            "141/141 - 42s - loss: 1.8242 - val_loss: 1.7773\n",
            "\n",
            "Epoch 00016: val_loss improved from 1.88388 to 1.77733, saving model to model.h5\n",
            "Epoch 17/200\n",
            "141/141 - 42s - loss: 1.7224 - val_loss: 1.6920\n",
            "\n",
            "Epoch 00017: val_loss improved from 1.77733 to 1.69203, saving model to model.h5\n",
            "Epoch 18/200\n",
            "141/141 - 44s - loss: 1.6267 - val_loss: 1.6023\n",
            "\n",
            "Epoch 00018: val_loss improved from 1.69203 to 1.60227, saving model to model.h5\n",
            "Epoch 19/200\n",
            "141/141 - 42s - loss: 1.5337 - val_loss: 1.5151\n",
            "\n",
            "Epoch 00019: val_loss improved from 1.60227 to 1.51509, saving model to model.h5\n",
            "Epoch 20/200\n",
            "141/141 - 43s - loss: 1.4439 - val_loss: 1.4371\n",
            "\n",
            "Epoch 00020: val_loss improved from 1.51509 to 1.43706, saving model to model.h5\n",
            "Epoch 21/200\n",
            "141/141 - 43s - loss: 1.3588 - val_loss: 1.3567\n",
            "\n",
            "Epoch 00021: val_loss improved from 1.43706 to 1.35670, saving model to model.h5\n",
            "Epoch 22/200\n",
            "141/141 - 43s - loss: 1.2764 - val_loss: 1.2842\n",
            "\n",
            "Epoch 00022: val_loss improved from 1.35670 to 1.28419, saving model to model.h5\n",
            "Epoch 23/200\n",
            "141/141 - 42s - loss: 1.1979 - val_loss: 1.2351\n",
            "\n",
            "Epoch 00023: val_loss improved from 1.28419 to 1.23513, saving model to model.h5\n",
            "Epoch 24/200\n",
            "141/141 - 42s - loss: 1.1219 - val_loss: 1.1482\n",
            "\n",
            "Epoch 00024: val_loss improved from 1.23513 to 1.14819, saving model to model.h5\n",
            "Epoch 25/200\n",
            "141/141 - 42s - loss: 1.0494 - val_loss: 1.0830\n",
            "\n",
            "Epoch 00025: val_loss improved from 1.14819 to 1.08302, saving model to model.h5\n",
            "Epoch 26/200\n",
            "141/141 - 42s - loss: 0.9806 - val_loss: 1.0242\n",
            "\n",
            "Epoch 00026: val_loss improved from 1.08302 to 1.02419, saving model to model.h5\n",
            "Epoch 27/200\n",
            "141/141 - 42s - loss: 0.9162 - val_loss: 0.9644\n",
            "\n",
            "Epoch 00027: val_loss improved from 1.02419 to 0.96444, saving model to model.h5\n",
            "Epoch 28/200\n",
            "141/141 - 42s - loss: 0.8514 - val_loss: 0.9091\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.96444 to 0.90906, saving model to model.h5\n",
            "Epoch 29/200\n",
            "141/141 - 43s - loss: 0.7896 - val_loss: 0.8559\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.90906 to 0.85591, saving model to model.h5\n",
            "Epoch 30/200\n",
            "141/141 - 43s - loss: 0.7342 - val_loss: 0.8190\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.85591 to 0.81902, saving model to model.h5\n",
            "Epoch 31/200\n",
            "141/141 - 42s - loss: 0.6830 - val_loss: 0.7675\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.81902 to 0.76748, saving model to model.h5\n",
            "Epoch 32/200\n",
            "141/141 - 42s - loss: 0.6379 - val_loss: 0.7305\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.76748 to 0.73050, saving model to model.h5\n",
            "Epoch 33/200\n",
            "141/141 - 42s - loss: 0.5916 - val_loss: 0.6956\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.73050 to 0.69558, saving model to model.h5\n",
            "Epoch 34/200\n",
            "141/141 - 43s - loss: 0.5469 - val_loss: 0.6612\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.69558 to 0.66119, saving model to model.h5\n",
            "Epoch 35/200\n",
            "141/141 - 43s - loss: 0.5096 - val_loss: 0.6502\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.66119 to 0.65023, saving model to model.h5\n",
            "Epoch 36/200\n",
            "141/141 - 42s - loss: 0.4783 - val_loss: 0.5983\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.65023 to 0.59831, saving model to model.h5\n",
            "Epoch 37/200\n",
            "141/141 - 43s - loss: 0.4388 - val_loss: 0.5668\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.59831 to 0.56685, saving model to model.h5\n",
            "Epoch 38/200\n",
            "141/141 - 43s - loss: 0.4082 - val_loss: 0.5464\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.56685 to 0.54644, saving model to model.h5\n",
            "Epoch 39/200\n",
            "141/141 - 42s - loss: 0.3796 - val_loss: 0.5181\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.54644 to 0.51810, saving model to model.h5\n",
            "Epoch 40/200\n",
            "141/141 - 42s - loss: 0.3528 - val_loss: 0.5034\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.51810 to 0.50342, saving model to model.h5\n",
            "Epoch 41/200\n",
            "141/141 - 43s - loss: 0.3283 - val_loss: 0.4846\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.50342 to 0.48462, saving model to model.h5\n",
            "Epoch 42/200\n",
            "141/141 - 42s - loss: 0.3048 - val_loss: 0.4605\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.48462 to 0.46055, saving model to model.h5\n",
            "Epoch 43/200\n",
            "141/141 - 42s - loss: 0.2858 - val_loss: 0.4451\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.46055 to 0.44511, saving model to model.h5\n",
            "Epoch 44/200\n",
            "141/141 - 43s - loss: 0.2654 - val_loss: 0.4341\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.44511 to 0.43412, saving model to model.h5\n",
            "Epoch 45/200\n",
            "141/141 - 43s - loss: 0.2485 - val_loss: 0.4186\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.43412 to 0.41856, saving model to model.h5\n",
            "Epoch 46/200\n",
            "141/141 - 43s - loss: 0.2319 - val_loss: 0.4048\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.41856 to 0.40480, saving model to model.h5\n",
            "Epoch 47/200\n",
            "141/141 - 42s - loss: 0.2177 - val_loss: 0.3951\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.40480 to 0.39506, saving model to model.h5\n",
            "Epoch 48/200\n",
            "141/141 - 42s - loss: 0.2040 - val_loss: 0.3841\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.39506 to 0.38415, saving model to model.h5\n",
            "Epoch 49/200\n",
            "141/141 - 42s - loss: 0.1940 - val_loss: 0.3775\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.38415 to 0.37750, saving model to model.h5\n",
            "Epoch 50/200\n",
            "141/141 - 42s - loss: 0.1819 - val_loss: 0.3697\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.37750 to 0.36967, saving model to model.h5\n",
            "Epoch 51/200\n",
            "141/141 - 43s - loss: 0.1717 - val_loss: 0.3641\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.36967 to 0.36406, saving model to model.h5\n",
            "Epoch 52/200\n",
            "141/141 - 42s - loss: 0.1634 - val_loss: 0.3548\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.36406 to 0.35481, saving model to model.h5\n",
            "Epoch 53/200\n",
            "141/141 - 42s - loss: 0.1526 - val_loss: 0.3474\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.35481 to 0.34736, saving model to model.h5\n",
            "Epoch 54/200\n",
            "141/141 - 42s - loss: 0.1453 - val_loss: 0.3409\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.34736 to 0.34095, saving model to model.h5\n",
            "Epoch 55/200\n",
            "141/141 - 43s - loss: 0.1384 - val_loss: 0.3349\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.34095 to 0.33493, saving model to model.h5\n",
            "Epoch 56/200\n",
            "141/141 - 42s - loss: 0.1299 - val_loss: 0.3312\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.33493 to 0.33123, saving model to model.h5\n",
            "Epoch 57/200\n",
            "141/141 - 42s - loss: 0.1240 - val_loss: 0.3282\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.33123 to 0.32821, saving model to model.h5\n",
            "Epoch 58/200\n",
            "141/141 - 43s - loss: 0.1183 - val_loss: 0.3240\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.32821 to 0.32403, saving model to model.h5\n",
            "Epoch 59/200\n",
            "141/141 - 43s - loss: 0.1135 - val_loss: 0.3219\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.32403 to 0.32192, saving model to model.h5\n",
            "Epoch 60/200\n",
            "141/141 - 42s - loss: 0.1119 - val_loss: 0.3201\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.32192 to 0.32007, saving model to model.h5\n",
            "Epoch 61/200\n",
            "141/141 - 42s - loss: 0.1072 - val_loss: 0.3189\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.32007 to 0.31886, saving model to model.h5\n",
            "Epoch 62/200\n",
            "141/141 - 43s - loss: 0.1064 - val_loss: 0.3150\n",
            "\n",
            "Epoch 00062: val_loss improved from 0.31886 to 0.31502, saving model to model.h5\n",
            "Epoch 63/200\n",
            "141/141 - 42s - loss: 0.1004 - val_loss: 0.3133\n",
            "\n",
            "Epoch 00063: val_loss improved from 0.31502 to 0.31334, saving model to model.h5\n",
            "Epoch 64/200\n",
            "141/141 - 42s - loss: 0.1008 - val_loss: 0.3136\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.31334\n",
            "Epoch 65/200\n",
            "141/141 - 42s - loss: 0.0986 - val_loss: 0.3084\n",
            "\n",
            "Epoch 00065: val_loss improved from 0.31334 to 0.30835, saving model to model.h5\n",
            "Epoch 66/200\n",
            "141/141 - 43s - loss: 0.0937 - val_loss: 0.3148\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.30835\n",
            "Epoch 67/200\n",
            "141/141 - 42s - loss: 0.0919 - val_loss: 0.3088\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.30835\n",
            "Epoch 68/200\n",
            "141/141 - 42s - loss: 0.0916 - val_loss: 0.3091\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.30835\n",
            "Epoch 69/200\n",
            "141/141 - 42s - loss: 0.0888 - val_loss: 0.3068\n",
            "\n",
            "Epoch 00069: val_loss improved from 0.30835 to 0.30680, saving model to model.h5\n",
            "Epoch 70/200\n",
            "141/141 - 42s - loss: 0.0859 - val_loss: 0.3050\n",
            "\n",
            "Epoch 00070: val_loss improved from 0.30680 to 0.30497, saving model to model.h5\n",
            "Epoch 71/200\n",
            "141/141 - 42s - loss: 0.0821 - val_loss: 0.3019\n",
            "\n",
            "Epoch 00071: val_loss improved from 0.30497 to 0.30195, saving model to model.h5\n",
            "Epoch 72/200\n",
            "141/141 - 42s - loss: 0.0822 - val_loss: 0.3025\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.30195\n",
            "Epoch 73/200\n",
            "141/141 - 43s - loss: 0.0792 - val_loss: 0.3000\n",
            "\n",
            "Epoch 00073: val_loss improved from 0.30195 to 0.30004, saving model to model.h5\n",
            "Epoch 74/200\n",
            "141/141 - 43s - loss: 0.0781 - val_loss: 0.3033\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.30004\n",
            "Epoch 75/200\n",
            "141/141 - 42s - loss: 0.0763 - val_loss: 0.3000\n",
            "\n",
            "Epoch 00075: val_loss improved from 0.30004 to 0.30002, saving model to model.h5\n",
            "Epoch 76/200\n",
            "141/141 - 42s - loss: 0.0757 - val_loss: 0.3032\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.30002\n",
            "Epoch 77/200\n",
            "141/141 - 42s - loss: 0.0772 - val_loss: 0.3030\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.30002\n",
            "Epoch 78/200\n",
            "141/141 - 42s - loss: 0.0779 - val_loss: 0.3068\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.30002\n",
            "Epoch 79/200\n",
            "141/141 - 42s - loss: 0.0772 - val_loss: 0.3052\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.30002\n",
            "Epoch 80/200\n",
            "141/141 - 42s - loss: 0.0750 - val_loss: 0.3049\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.30002\n",
            "Epoch 81/200\n",
            "141/141 - 42s - loss: 0.0763 - val_loss: 0.3033\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.30002\n",
            "Epoch 82/200\n",
            "141/141 - 42s - loss: 0.0744 - val_loss: 0.3050\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.30002\n",
            "Epoch 83/200\n",
            "141/141 - 42s - loss: 0.0720 - val_loss: 0.2984\n",
            "\n",
            "Epoch 00083: val_loss improved from 0.30002 to 0.29842, saving model to model.h5\n",
            "Epoch 84/200\n",
            "141/141 - 42s - loss: 0.0689 - val_loss: 0.2970\n",
            "\n",
            "Epoch 00084: val_loss improved from 0.29842 to 0.29699, saving model to model.h5\n",
            "Epoch 85/200\n",
            "141/141 - 42s - loss: 0.0674 - val_loss: 0.2971\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.29699\n",
            "Epoch 86/200\n",
            "141/141 - 42s - loss: 0.0663 - val_loss: 0.2981\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.29699\n",
            "Epoch 87/200\n",
            "141/141 - 42s - loss: 0.0665 - val_loss: 0.2972\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.29699\n",
            "Epoch 88/200\n",
            "141/141 - 42s - loss: 0.0660 - val_loss: 0.3040\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.29699\n",
            "Epoch 89/200\n",
            "141/141 - 43s - loss: 0.0695 - val_loss: 0.3040\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.29699\n",
            "Epoch 90/200\n",
            "141/141 - 42s - loss: 0.0699 - val_loss: 0.3050\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.29699\n",
            "Epoch 91/200\n",
            "141/141 - 42s - loss: 0.0728 - val_loss: 0.3100\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.29699\n",
            "Epoch 92/200\n",
            "141/141 - 42s - loss: 0.0819 - val_loss: 0.3164\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.29699\n",
            "Epoch 93/200\n",
            "141/141 - 42s - loss: 0.0790 - val_loss: 0.3075\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.29699\n",
            "Epoch 94/200\n",
            "141/141 - 42s - loss: 0.0752 - val_loss: 0.3061\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.29699\n",
            "Epoch 95/200\n",
            "141/141 - 42s - loss: 0.0710 - val_loss: 0.3010\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.29699\n",
            "Epoch 96/200\n",
            "141/141 - 42s - loss: 0.0650 - val_loss: 0.2987\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.29699\n",
            "Epoch 97/200\n",
            "141/141 - 42s - loss: 0.0634 - val_loss: 0.2988\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.29699\n",
            "Epoch 98/200\n",
            "141/141 - 42s - loss: 0.0615 - val_loss: 0.2985\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.29699\n",
            "Epoch 99/200\n",
            "141/141 - 42s - loss: 0.0608 - val_loss: 0.2975\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.29699\n",
            "Epoch 100/200\n",
            "141/141 - 42s - loss: 0.0598 - val_loss: 0.2971\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.29699\n",
            "Epoch 101/200\n",
            "141/141 - 42s - loss: 0.0592 - val_loss: 0.2987\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.29699\n",
            "Epoch 102/200\n",
            "141/141 - 42s - loss: 0.0590 - val_loss: 0.2971\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.29699\n",
            "Epoch 103/200\n",
            "141/141 - 42s - loss: 0.0593 - val_loss: 0.2982\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.29699\n",
            "Epoch 104/200\n",
            "141/141 - 42s - loss: 0.0591 - val_loss: 0.2974\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.29699\n",
            "Epoch 105/200\n",
            "141/141 - 42s - loss: 0.0592 - val_loss: 0.2995\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.29699\n",
            "Epoch 106/200\n",
            "141/141 - 42s - loss: 0.0599 - val_loss: 0.3027\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.29699\n",
            "Epoch 107/200\n",
            "141/141 - 42s - loss: 0.0607 - val_loss: 0.3016\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.29699\n",
            "Epoch 108/200\n",
            "141/141 - 42s - loss: 0.0617 - val_loss: 0.3049\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.29699\n",
            "Epoch 109/200\n",
            "141/141 - 42s - loss: 0.0650 - val_loss: 0.3059\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.29699\n",
            "Epoch 110/200\n",
            "141/141 - 42s - loss: 0.0819 - val_loss: 0.3480\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.29699\n",
            "Epoch 111/200\n",
            "141/141 - 42s - loss: 0.1060 - val_loss: 0.3255\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.29699\n",
            "Epoch 112/200\n",
            "141/141 - 42s - loss: 0.0823 - val_loss: 0.3135\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.29699\n",
            "Epoch 113/200\n",
            "141/141 - 42s - loss: 0.0690 - val_loss: 0.3020\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.29699\n",
            "Epoch 114/200\n",
            "141/141 - 42s - loss: 0.0599 - val_loss: 0.3000\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.29699\n",
            "Epoch 115/200\n",
            "141/141 - 42s - loss: 0.0571 - val_loss: 0.2986\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.29699\n",
            "Epoch 116/200\n",
            "141/141 - 42s - loss: 0.0563 - val_loss: 0.2977\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.29699\n",
            "Epoch 117/200\n",
            "141/141 - 42s - loss: 0.0555 - val_loss: 0.2984\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.29699\n",
            "Epoch 118/200\n",
            "141/141 - 42s - loss: 0.0552 - val_loss: 0.3000\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.29699\n",
            "Epoch 119/200\n",
            "141/141 - 42s - loss: 0.0546 - val_loss: 0.2998\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.29699\n",
            "Epoch 120/200\n",
            "141/141 - 42s - loss: 0.0550 - val_loss: 0.3000\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.29699\n",
            "Epoch 121/200\n",
            "141/141 - 42s - loss: 0.0552 - val_loss: 0.2991\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.29699\n",
            "Epoch 122/200\n",
            "141/141 - 42s - loss: 0.0554 - val_loss: 0.3005\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.29699\n",
            "Epoch 123/200\n",
            "141/141 - 42s - loss: 0.0563 - val_loss: 0.3005\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.29699\n",
            "Epoch 124/200\n",
            "141/141 - 42s - loss: 0.0561 - val_loss: 0.3034\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.29699\n",
            "Epoch 125/200\n",
            "141/141 - 42s - loss: 0.0569 - val_loss: 0.3025\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.29699\n",
            "Epoch 126/200\n",
            "141/141 - 42s - loss: 0.0570 - val_loss: 0.3020\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.29699\n",
            "Epoch 127/200\n",
            "141/141 - 42s - loss: 0.0564 - val_loss: 0.3021\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.29699\n",
            "Epoch 128/200\n",
            "141/141 - 42s - loss: 0.0565 - val_loss: 0.3030\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.29699\n",
            "Epoch 129/200\n",
            "141/141 - 42s - loss: 0.0566 - val_loss: 0.3014\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.29699\n",
            "Epoch 130/200\n",
            "141/141 - 42s - loss: 0.0562 - val_loss: 0.3037\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.29699\n",
            "Epoch 131/200\n",
            "141/141 - 42s - loss: 0.0562 - val_loss: 0.3024\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.29699\n",
            "Epoch 132/200\n",
            "141/141 - 42s - loss: 0.0571 - val_loss: 0.3054\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.29699\n",
            "Epoch 133/200\n",
            "141/141 - 42s - loss: 0.0570 - val_loss: 0.3043\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.29699\n",
            "Epoch 134/200\n",
            "141/141 - 42s - loss: 0.0595 - val_loss: 0.3120\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.29699\n",
            "Epoch 135/200\n",
            "141/141 - 42s - loss: 0.0810 - val_loss: 0.3362\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.29699\n",
            "Epoch 136/200\n",
            "141/141 - 42s - loss: 0.0908 - val_loss: 0.3243\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.29699\n",
            "Epoch 137/200\n",
            "141/141 - 42s - loss: 0.0742 - val_loss: 0.3115\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.29699\n",
            "Epoch 138/200\n",
            "141/141 - 42s - loss: 0.0641 - val_loss: 0.3056\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.29699\n",
            "Epoch 139/200\n",
            "141/141 - 42s - loss: 0.0575 - val_loss: 0.3024\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.29699\n",
            "Epoch 140/200\n",
            "141/141 - 42s - loss: 0.0548 - val_loss: 0.3003\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.29699\n",
            "Epoch 141/200\n",
            "141/141 - 42s - loss: 0.0535 - val_loss: 0.3013\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.29699\n",
            "Epoch 142/200\n",
            "141/141 - 42s - loss: 0.0526 - val_loss: 0.3010\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.29699\n",
            "Epoch 143/200\n",
            "141/141 - 42s - loss: 0.0528 - val_loss: 0.3014\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.29699\n",
            "Epoch 144/200\n",
            "141/141 - 42s - loss: 0.0524 - val_loss: 0.3022\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.29699\n",
            "Epoch 145/200\n",
            "141/141 - 42s - loss: 0.0525 - val_loss: 0.3022\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.29699\n",
            "Epoch 146/200\n",
            "141/141 - 43s - loss: 0.0527 - val_loss: 0.3022\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.29699\n",
            "Epoch 147/200\n",
            "141/141 - 42s - loss: 0.0528 - val_loss: 0.3041\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.29699\n",
            "Epoch 148/200\n",
            "141/141 - 43s - loss: 0.0530 - val_loss: 0.3030\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.29699\n",
            "Epoch 149/200\n",
            "141/141 - 43s - loss: 0.0532 - val_loss: 0.3046\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.29699\n",
            "Epoch 150/200\n",
            "141/141 - 43s - loss: 0.0535 - val_loss: 0.3059\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.29699\n",
            "Epoch 151/200\n",
            "141/141 - 43s - loss: 0.0537 - val_loss: 0.3051\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.29699\n",
            "Epoch 152/200\n",
            "141/141 - 42s - loss: 0.0534 - val_loss: 0.3056\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.29699\n",
            "Epoch 153/200\n",
            "141/141 - 43s - loss: 0.0536 - val_loss: 0.3054\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.29699\n",
            "Epoch 154/200\n",
            "141/141 - 42s - loss: 0.0538 - val_loss: 0.3053\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.29699\n",
            "Epoch 155/200\n",
            "141/141 - 42s - loss: 0.0537 - val_loss: 0.3073\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.29699\n",
            "Epoch 156/200\n",
            "141/141 - 43s - loss: 0.0550 - val_loss: 0.3075\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.29699\n",
            "Epoch 157/200\n",
            "141/141 - 42s - loss: 0.0555 - val_loss: 0.3101\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.29699\n",
            "Epoch 158/200\n",
            "141/141 - 42s - loss: 0.0644 - val_loss: 0.3351\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.29699\n",
            "Epoch 159/200\n",
            "141/141 - 42s - loss: 0.0917 - val_loss: 0.3369\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.29699\n",
            "Epoch 160/200\n",
            "141/141 - 43s - loss: 0.0820 - val_loss: 0.3203\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.29699\n",
            "Epoch 161/200\n",
            "141/141 - 43s - loss: 0.0672 - val_loss: 0.3103\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.29699\n",
            "Epoch 162/200\n",
            "141/141 - 43s - loss: 0.0576 - val_loss: 0.3067\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.29699\n",
            "Epoch 163/200\n",
            "141/141 - 43s - loss: 0.0535 - val_loss: 0.3050\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.29699\n",
            "Epoch 164/200\n",
            "141/141 - 43s - loss: 0.0523 - val_loss: 0.3061\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.29699\n",
            "Epoch 165/200\n",
            "141/141 - 43s - loss: 0.0519 - val_loss: 0.3050\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.29699\n",
            "Epoch 166/200\n",
            "141/141 - 43s - loss: 0.0514 - val_loss: 0.3059\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.29699\n",
            "Epoch 167/200\n",
            "141/141 - 43s - loss: 0.0514 - val_loss: 0.3055\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.29699\n",
            "Epoch 168/200\n",
            "141/141 - 43s - loss: 0.0510 - val_loss: 0.3061\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.29699\n",
            "Epoch 169/200\n",
            "141/141 - 43s - loss: 0.0512 - val_loss: 0.3066\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.29699\n",
            "Epoch 170/200\n",
            "141/141 - 43s - loss: 0.0512 - val_loss: 0.3068\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.29699\n",
            "Epoch 171/200\n",
            "141/141 - 43s - loss: 0.0514 - val_loss: 0.3072\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.29699\n",
            "Epoch 172/200\n",
            "141/141 - 43s - loss: 0.0514 - val_loss: 0.3070\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.29699\n",
            "Epoch 173/200\n",
            "141/141 - 42s - loss: 0.0512 - val_loss: 0.3073\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.29699\n",
            "Epoch 174/200\n",
            "141/141 - 42s - loss: 0.0517 - val_loss: 0.3074\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.29699\n",
            "Epoch 175/200\n",
            "141/141 - 43s - loss: 0.0517 - val_loss: 0.3075\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.29699\n",
            "Epoch 176/200\n",
            "141/141 - 42s - loss: 0.0523 - val_loss: 0.3074\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.29699\n",
            "Epoch 177/200\n",
            "141/141 - 42s - loss: 0.0523 - val_loss: 0.3089\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.29699\n",
            "Epoch 178/200\n",
            "141/141 - 43s - loss: 0.0522 - val_loss: 0.3084\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.29699\n",
            "Epoch 179/200\n",
            "141/141 - 42s - loss: 0.0524 - val_loss: 0.3080\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.29699\n",
            "Epoch 180/200\n",
            "141/141 - 42s - loss: 0.0526 - val_loss: 0.3081\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.29699\n",
            "Epoch 181/200\n",
            "141/141 - 42s - loss: 0.0530 - val_loss: 0.3104\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.29699\n",
            "Epoch 182/200\n",
            "141/141 - 42s - loss: 0.0532 - val_loss: 0.3129\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.29699\n",
            "Epoch 183/200\n",
            "141/141 - 42s - loss: 0.0548 - val_loss: 0.3148\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.29699\n",
            "Epoch 184/200\n",
            "141/141 - 42s - loss: 0.0639 - val_loss: 0.3298\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.29699\n",
            "Epoch 185/200\n",
            "141/141 - 42s - loss: 0.0855 - val_loss: 0.3312\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.29699\n",
            "Epoch 186/200\n",
            "141/141 - 42s - loss: 0.0771 - val_loss: 0.3190\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.29699\n",
            "Epoch 187/200\n",
            "141/141 - 42s - loss: 0.0611 - val_loss: 0.3110\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.29699\n",
            "Epoch 188/200\n",
            "141/141 - 42s - loss: 0.0541 - val_loss: 0.3088\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.29699\n",
            "Epoch 189/200\n",
            "141/141 - 42s - loss: 0.0523 - val_loss: 0.3074\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.29699\n",
            "Epoch 190/200\n",
            "141/141 - 42s - loss: 0.0511 - val_loss: 0.3072\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.29699\n",
            "Epoch 191/200\n",
            "141/141 - 42s - loss: 0.0504 - val_loss: 0.3070\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.29699\n",
            "Epoch 192/200\n",
            "141/141 - 42s - loss: 0.0507 - val_loss: 0.3075\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.29699\n",
            "Epoch 193/200\n",
            "141/141 - 42s - loss: 0.0505 - val_loss: 0.3077\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.29699\n",
            "Epoch 194/200\n",
            "141/141 - 42s - loss: 0.0500 - val_loss: 0.3087\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.29699\n",
            "Epoch 195/200\n",
            "141/141 - 42s - loss: 0.0505 - val_loss: 0.3082\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.29699\n",
            "Epoch 196/200\n",
            "141/141 - 42s - loss: 0.0502 - val_loss: 0.3093\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.29699\n",
            "Epoch 197/200\n",
            "141/141 - 42s - loss: 0.0505 - val_loss: 0.3096\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.29699\n",
            "Epoch 198/200\n",
            "141/141 - 42s - loss: 0.0507 - val_loss: 0.3093\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.29699\n",
            "Epoch 199/200\n",
            "141/141 - 42s - loss: 0.0511 - val_loss: 0.3104\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.29699\n",
            "Epoch 200/200\n",
            "141/141 - 42s - loss: 0.0506 - val_loss: 0.3111\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.29699\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f2e44ebec50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BoEKOA61hePN",
        "outputId": "c171a88a-ab76-4a0c-c4a1-dd35427008f4"
      },
      "source": [
        "\n",
        "from pickle import load\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None\n",
        "\n",
        "# generate target given source sequence\n",
        "def predict_sequence(model, tokenizer, source):\n",
        "\tprediction = model.predict(source, verbose=0)[0]\n",
        "\tintegers = [argmax(vector) for vector in prediction]\n",
        "\ttarget = list()\n",
        "\tfor i in integers:\n",
        "\t\tword = word_for_id(i, tokenizer)\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\ttarget.append(word)\n",
        "\treturn ' '.join(target)\n",
        "\n",
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
        "\tactual, predicted = list(), list()\n",
        "\tfor i, source in enumerate(sources):\n",
        "\t\t# translate encoded source text\n",
        "\t\tsource = source.reshape((1, source.shape[0]))\n",
        "\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n",
        "\t\traw_target, raw_src,test = raw_dataset[i]\n",
        "\t\tif i < 100:\n",
        "\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
        "\t\tactual.append([raw_target.split()])\n",
        "\t\tpredicted.append(translation.split())\n",
        "\t# calculate BLEU score\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-danish-both.pkl')\n",
        "train = load_clean_sentences('english-danish-train.pkl')\n",
        "test = load_clean_sentences('english-danish-test.pkl')\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "# prepare data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "\n",
        "# load model\n",
        "model = load_model('model.h5')\n",
        "# test on some training sequences\n",
        "print('train')\n",
        "evaluate_model(model, eng_tokenizer, trainX, train)\n",
        "# test on some test sequences\n",
        "print('test')\n",
        "evaluate_model(model, eng_tokenizer, testX, test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train\n",
            "src=[giv mig mine penge tilbage], target=[give me my money back], predicted=[give me my money back]\n",
            "src=[tom er min bedste ven], target=[tom is my best friend], predicted=[tom is my best friend]\n",
            "src=[tom er meget gammel], target=[tom is very old], predicted=[tom is very old]\n",
            "src=[tom har ingen sociale frdigheder], target=[tom has no social skills], predicted=[tom has no social skills]\n",
            "src=[det var sort], target=[it was black], predicted=[it was black]\n",
            "src=[er mit vasketj klar], target=[is my laundry ready], predicted=[is my laundry ready]\n",
            "src=[det er et frygteligt problem], target=[its a terrible problem], predicted=[its a terrible problem]\n",
            "src=[tnk over det], target=[think about it], predicted=[think about it]\n",
            "src=[er der vand pa mars], target=[is there water on mars], predicted=[is there water on mars]\n",
            "src=[lad mig vre i fred], target=[leave me alone], predicted=[leave me alone]\n",
            "src=[ingen havde et svar], target=[no one had an answer], predicted=[no one an an answer]\n",
            "src=[tom dansede], target=[tom danced], predicted=[tom danced]\n",
            "src=[denne kaffebar er hyggelig], target=[this coffee shop is cozy], predicted=[this coffee shop is cozy]\n",
            "src=[tag venligst et bad], target=[please take a bath], predicted=[please take a bath]\n",
            "src=[tom fik tre l], target=[tom had three beers], predicted=[tom had three beers]\n",
            "src=[han er en hj dreng], target=[he is a tall boy], predicted=[he a a boy]\n",
            "src=[tom vil blive imponeret], target=[tom will be impressed], predicted=[tom will be impressed]\n",
            "src=[det rager mig en papand], target=[i couldnt care less], predicted=[i couldnt care less]\n",
            "src=[min mor er overbeskyttende], target=[my mom is overprotective], predicted=[my mom is overprotective]\n",
            "src=[hold op med at ryge], target=[stop smoking], predicted=[stop smoking]\n",
            "src=[tom arbejder], target=[tom works], predicted=[tom is]\n",
            "src=[bananer er gule], target=[bananas are yellow], predicted=[bananas are yellow]\n",
            "src=[tom kaldte sin hund for cookie], target=[tom named his dog cookie], predicted=[tom named his dog cookie]\n",
            "src=[jeg fik ingen rabat], target=[i didnt get a discount], predicted=[i didnt get a discount]\n",
            "src=[han kan ikke tage en beslutning], target=[he cant make a decision], predicted=[he cant make a decision]\n",
            "src=[du behver ikke at lyve], target=[you dont have to lie], predicted=[you dont have to lie]\n",
            "src=[jeg troede tom var rig], target=[i thought tom was rich], predicted=[i thought tom was rich]\n",
            "src=[lad mig vide hvordan det gar], target=[let me know how it goes], predicted=[let me know how it goes]\n",
            "src=[det er en sammensvrgelse], target=[its a conspiracy], predicted=[its a conspiracy]\n",
            "src=[jeg ser tv], target=[i watch television], predicted=[i watch television]\n",
            "src=[hvad er dit telefonnummer], target=[whats your phone number], predicted=[whats your phone number]\n",
            "src=[jeres hus er stort], target=[your house is big], predicted=[your house is big]\n",
            "src=[hvad foregar der herinde], target=[whats going on in here], predicted=[whats going on in here]\n",
            "src=[det ser ud til regn i dag], target=[it looks like rain today], predicted=[it looks like rain today]\n",
            "src=[det er derfor han blev vred], target=[thats why he got angry], predicted=[thats why he got angry]\n",
            "src=[jeg elsker dette album], target=[i love this album], predicted=[i love this album]\n",
            "src=[jeg danser], target=[im dancing], predicted=[im dancing]\n",
            "src=[hvad betyder usb], target=[what does usb stand for], predicted=[what does usb stand for]\n",
            "src=[denne bog er meget lille], target=[this book is very small], predicted=[this book is very small]\n",
            "src=[jeg vil gerne have dig til at blive], target=[i want you to stay], predicted=[i want you to stay]\n",
            "src=[las dren], target=[lock the door], predicted=[lock the door]\n",
            "src=[hun har en hvid kat], target=[she has a white cat], predicted=[she has a white cat]\n",
            "src=[vi er udenfor fare], target=[were out of danger], predicted=[were out of danger]\n",
            "src=[du runder de tredive], target=[youre turning thirty], predicted=[youre turning thirty]\n",
            "src=[tom sad pa en trstamme], target=[tom sat on a log], predicted=[tom sat on a log]\n",
            "src=[jeg skal sove], target=[i have to sleep], predicted=[i have to sleep]\n",
            "src=[ikke alle fugle er i stand til at flyve], target=[not all birds can fly], predicted=[not all birds can fly]\n",
            "src=[lad os prve noget], target=[lets try something], predicted=[lets try something]\n",
            "src=[lige et jeblik], target=[just a moment], predicted=[just a moment]\n",
            "src=[tom er i smult vande], target=[tom is out of the woods], predicted=[tom is out of the woods]\n",
            "src=[det her er meget salt], target=[this is very salty], predicted=[this is very salty]\n",
            "src=[jeg er sulten], target=[im hungry], predicted=[im hungry]\n",
            "src=[er jeg i sikkerhed nu], target=[am i safe now], predicted=[am i safe now]\n",
            "src=[jeg kender ikke sandheden], target=[i dont know the truth], predicted=[i dont know the truth]\n",
            "src=[jeg ringede pa klokken], target=[i rang the bell], predicted=[i rang the bell]\n",
            "src=[det var kun for sjov], target=[im only joking], predicted=[im only only joking]\n",
            "src=[tom var fransklrer], target=[tom was a french teacher], predicted=[tom was a french teacher]\n",
            "src=[det er min hest], target=[its my horse], predicted=[its my horse]\n",
            "src=[jeg holder af at fiske efter rred], target=[i like trout fishing], predicted=[i like trout fishing]\n",
            "src=[hvor lagde jeg mine ngler], target=[where did i put my keys], predicted=[where did i put my keys]\n",
            "src=[hold venligst op med at fljte], target=[please stop whistling], predicted=[please stop whistling]\n",
            "src=[tom skd genvej], target=[tom took a short cut], predicted=[tom took a short cut]\n",
            "src=[bor du i omradet], target=[do you live in the area], predicted=[do you live in the area]\n",
            "src=[jeg har brug for en sav], target=[i need a saw], predicted=[i need a saw]\n",
            "src=[han er altid forberedt], target=[he is always prepared], predicted=[he is always prepared]\n",
            "src=[hun elsker tom ikke mig], target=[she loves tom not me], predicted=[she loves tom not me]\n",
            "src=[du er kun en dreng], target=[youre just a boy], predicted=[youre just a boy]\n",
            "src=[de blev gift], target=[they got married], predicted=[they got married]\n",
            "src=[er tom ikke musiker], target=[isnt tom a musician], predicted=[isnt tom a musician]\n",
            "src=[kan jeg regne med din hjlp], target=[can i count on your help], predicted=[can i count on your help]\n",
            "src=[tom er biavler], target=[tom is a beekeeper], predicted=[tom is a beekeeper]\n",
            "src=[er din bil hurtig], target=[is your car fast], predicted=[is your car fast]\n",
            "src=[jeg kan godt lide din ny frisure], target=[i like your new hairstyle], predicted=[i like your new hairstyle]\n",
            "src=[dinosaurusser er nu uddde], target=[dinosaurs are now extinct], predicted=[dinosaurs are now extinct]\n",
            "src=[hun er meget smuk], target=[she is very pretty], predicted=[she is very pretty]\n",
            "src=[tom er ved at koge et g], target=[tom is boiling an egg], predicted=[tom is boiling an egg]\n",
            "src=[hallj rr ikke ved noget], target=[hey dont touch anything], predicted=[hey dont touch anything]\n",
            "src=[du har lys pa], target=[you left your lights on], predicted=[you left your lights on]\n",
            "src=[hvorfor er de ikke bekymret], target=[why arent they worried], predicted=[why arent they worried]\n",
            "src=[jeg er elektriker], target=[i am an electrician], predicted=[i am an electrician]\n",
            "src=[jeg har brug for lidt alenetid], target=[i need some alone time], predicted=[i need some alone time]\n",
            "src=[tom mistede sin teddybjrn], target=[tom lost his teddy bear], predicted=[tom lost his teddy bear]\n",
            "src=[jeg kan lide at bygge broer], target=[i like to build bridges], predicted=[i like to build bridges]\n",
            "src=[rejser du alene], target=[are you traveling alone], predicted=[are you traveling alone]\n",
            "src=[jeg hrer ikke til her], target=[i dont belong here], predicted=[i dont belong here]\n",
            "src=[vrsagod at tage plads], target=[have a seat please], predicted=[have a seat please]\n",
            "src=[jeg har kbt en slow cooker], target=[i bought a slow cooker], predicted=[i bought a slow cooker]\n",
            "src=[det kommer du til at fortryde], target=[youll regret this], predicted=[youll regret this]\n",
            "src=[ga ind og vk tom], target=[go and wake tom up], predicted=[go and wake tom up]\n",
            "src=[du kan godt komme ind], target=[you can come in], predicted=[you can come in]\n",
            "src=[blerne er rde], target=[the apples are red], predicted=[the apples are red]\n",
            "src=[var i med til koncerten], target=[were you at the concert], predicted=[were you at the concert]\n",
            "src=[tom havde blomster med], target=[tom brought flowers], predicted=[tom brought flowers]\n",
            "src=[kan jeg fa din autograf], target=[can i get your autograph], predicted=[can i have your autograph]\n",
            "src=[natten er stadig ung], target=[the night is still young], predicted=[the nights is still]\n",
            "src=[jeg sa tv], target=[i was watching tv], predicted=[i was watching tv]\n",
            "src=[tl fra en til ti], target=[count from one to ten], predicted=[count from one to ten]\n",
            "src=[dette papir er groft], target=[this paper is rough], predicted=[this paper is rough]\n",
            "src=[dette ble er darligt], target=[this apple is bad], predicted=[this apple is bad]\n",
            "src=[jeg gav dig en bog], target=[i gave you a book], predicted=[i gave you a book]\n",
            "BLEU-1: 0.963545\n",
            "BLEU-2: 0.950380\n",
            "BLEU-3: 0.938953\n",
            "BLEU-4: 0.882369\n",
            "test\n",
            "src=[har du et dobbeltvrelse], target=[do you have a double room], predicted=[do you have a double room]\n",
            "src=[man siger at krligheden er blind], target=[they say love is blind], predicted=[they say love is blind]\n",
            "src=[han har hovedpine], target=[hes got a headache], predicted=[hes got a headache]\n",
            "src=[det er nok for i dag], target=[thats enough for today], predicted=[thats enough for today]\n",
            "src=[katte har ni liv], target=[cats have nine lives], predicted=[cats have nine lives]\n",
            "src=[den er], target=[its], predicted=[its eightthirty]\n",
            "src=[jeg er ikke fdt i gar], target=[i wasnt born yesterday], predicted=[i wasnt born yesterday]\n",
            "src=[hvem solgte denne bil til dig], target=[who sold you this car], predicted=[who sold you this car]\n",
            "src=[jeg foretrkker sort], target=[i prefer black], predicted=[i prefer black]\n",
            "src=[hvem har skrevet dette brev], target=[who wrote this letter], predicted=[who wrote this letter]\n",
            "src=[ma jeg ga pa toilettet], target=[may i go to the bathroom], predicted=[may i go to the toilet]\n",
            "src=[hvem sa han], target=[who did he see], predicted=[who did he see]\n",
            "src=[det tilhrer min bror], target=[its my brothers], predicted=[it belongs to my brother]\n",
            "src=[jeg gjorde det med vilje], target=[i did it on purpose], predicted=[i did it on purpose]\n",
            "src=[intet gik galt], target=[nothing went wrong], predicted=[nothing went wrong]\n",
            "src=[jeg er ikke ved at skrive et brev], target=[i am not writing a letter], predicted=[i am not writing a letter]\n",
            "src=[kongen blev henrettet], target=[the king was executed], predicted=[the king was executed]\n",
            "src=[vi lukkede kufferten], target=[we closed the suitcase], predicted=[we closed the suitcase]\n",
            "src=[tom sagde at du var kommet], target=[tom said you had come], predicted=[tom said youd youd come]\n",
            "src=[er din far lge], target=[is your father a doctor], predicted=[is your father a doctor]\n",
            "src=[jeg gar pa harvard], target=[i go to harvard], predicted=[i go to harvard]\n",
            "src=[tom du trnger til at blive klippet], target=[tom you need a haircut], predicted=[tom you need a haircut]\n",
            "src=[jeg ved hvor tom arbejder], target=[i know where tom works], predicted=[i know where tom works]\n",
            "src=[tom lrer mig fransk], target=[tom teaches me french], predicted=[tom teaches me french]\n",
            "src=[er hans navn ikke tom], target=[isnt his name tom], predicted=[isnt his name tom]\n",
            "src=[er dit navn tom], target=[is your name tom], predicted=[is your name tom]\n",
            "src=[tom er i seng], target=[tom is in bed], predicted=[tom is in bed]\n",
            "src=[tom er cirka tredive ar], target=[tom is around thirty], predicted=[tom is around thirty]\n",
            "src=[vgtstngerne er tunge], target=[the barbells are heavy], predicted=[the barbells are heavy]\n",
            "src=[luk ikke hunden ind], target=[dont let the dog in], predicted=[dont let the dog in]\n",
            "src=[jeg vil gerne tjekke ud nu], target=[i want to check out now], predicted=[i want to check out now]\n",
            "src=[det her er virkelig nemt], target=[this is real easy], predicted=[this is real easy]\n",
            "src=[bedre sent end aldrig], target=[better late than never], predicted=[better late than never]\n",
            "src=[vi stoler ikke pa dig], target=[we dont trust you], predicted=[we dont trust you]\n",
            "src=[jeg forstar ikke fransk], target=[i dont understand french], predicted=[i dont understand french]\n",
            "src=[det er hvad jeg sa], target=[thats what i saw], predicted=[thats what i saw]\n",
            "src=[tom er et vidunderbarn], target=[tom is a child prodigy], predicted=[tom is a child prodigy]\n",
            "src=[tom elsker bger], target=[tom loves books], predicted=[tom loves books]\n",
            "src=[du er en rigtig lgnhals], target=[you are such a liar], predicted=[you are a a liar]\n",
            "src=[hun er pa hospitalet nu], target=[shes in the hospital now], predicted=[shes in the hospital now]\n",
            "src=[tom har kbt en bil til mary], target=[tom bought mary a car], predicted=[tom bought mary a car]\n",
            "src=[det er et godt sprgsmal], target=[thats a good question], predicted=[its a good question]\n",
            "src=[jeg vil hre alt], target=[i want to hear everything], predicted=[i want to hear everything]\n",
            "src=[jeg brkkede benet], target=[i broke my leg], predicted=[i broke my leg]\n",
            "src=[jeg er ikke bermt], target=[im not famous], predicted=[im not famous]\n",
            "src=[tom hjalp], target=[tom helped], predicted=[tom helped out]\n",
            "src=[tag min bil], target=[take my car], predicted=[take my car]\n",
            "src=[det er kun mandag], target=[its only monday], predicted=[its only monday]\n",
            "src=[har du brn], target=[do you have any kids], predicted=[do you have kids kids]\n",
            "src=[sluk for lyden], target=[turn the sound off], predicted=[turn the sound off]\n",
            "src=[har i vanter], target=[do you have mittens], predicted=[do you have mittens]\n",
            "src=[tom er eksatlet], target=[tom is a former athlete], predicted=[tom is a former athlete]\n",
            "src=[det kan vi ikke garantere], target=[we cant guarantee that], predicted=[we cant guarantee that]\n",
            "src=[vi gav hinanden handen], target=[we shook hands], predicted=[we shook hands]\n",
            "src=[er det fransk de taler], target=[are they speaking french], predicted=[are they speaking french]\n",
            "src=[tom gik ombord pa flyet], target=[tom boarded the plane], predicted=[tom boarded the plane]\n",
            "src=[det er en af mine hunde], target=[thats one of my dogs], predicted=[thats one of my dogs]\n",
            "src=[hun har solbriller], target=[she has sunglasses], predicted=[she has sunglasses]\n",
            "src=[er du lge], target=[are you a doctor], predicted=[are you a doctor]\n",
            "src=[jeg er bedre end dig], target=[i am better than you], predicted=[i am better than you]\n",
            "src=[har i spist jeres spinat], target=[did you eat your spinach], predicted=[did you eat your spinach]\n",
            "src=[jeg tror pa spgelser], target=[i believe in ghosts], predicted=[i believe in ghosts]\n",
            "src=[hvad er asfalt lavet af], target=[whats asphalt made of], predicted=[whats asphalt made of]\n",
            "src=[tom ventede utalmodigt], target=[tom waited impatiently], predicted=[tom waited impatiently]\n",
            "src=[jeg sa tom spille tennis], target=[i saw tom play tennis], predicted=[i saw tom play tennis]\n",
            "src=[giv mig en kop kaffe], target=[get me a cup of coffee], predicted=[get me a cup of coffee]\n",
            "src=[jeg blev direktr], target=[i became a director], predicted=[i became a director]\n",
            "src=[hvem gjorde det], target=[who did it], predicted=[who did it]\n",
            "src=[vi slger ikke l], target=[we dont sell beer], predicted=[we dont sell beer]\n",
            "src=[tom ventede i bilen], target=[tom waited in the car], predicted=[tom waited in the car]\n",
            "src=[du kan parkere her], target=[you can park here], predicted=[you may park here]\n",
            "src=[jeg er ked af at jeg rabte ad dig], target=[im sorry i yelled at you], predicted=[im sorry i yelled at you]\n",
            "src=[det er sadan en smuk dag], target=[its such a beautiful day], predicted=[its such a beautiful day]\n",
            "src=[tom sover], target=[toms asleep], predicted=[toms asleep]\n",
            "src=[tom studerer], target=[toms studying], predicted=[tom studying studying]\n",
            "src=[jeg er lige staet op], target=[i just got up], predicted=[i just got up]\n",
            "src=[du vil takke mig senere], target=[youll thank me later], predicted=[youll thank me later]\n",
            "src=[mary er min ekskreste], target=[mary is my exgirlfriend], predicted=[mary is my exgirlfriend]\n",
            "src=[tom er en slapsvans], target=[tom is a wimp], predicted=[tom is a wimp]\n",
            "src=[de var begge berusede], target=[they were both drunk], predicted=[they were both drunk]\n",
            "src=[vores gteskab sluttede], target=[our marriage is over], predicted=[our marriage is over]\n",
            "src=[han leger derovre], target=[he is playing over there], predicted=[he is playing over there]\n",
            "src=[hans navn er tom], target=[his name is tom], predicted=[his name tom tom]\n",
            "src=[jeg er min egen direktr], target=[im my own boss], predicted=[im my own boss]\n",
            "src=[de fremsatte mange anklager], target=[they made many charges], predicted=[they made many charges]\n",
            "src=[hvad er det i gar og laver brn], target=[what are you kids doing], predicted=[what are you kids doing]\n",
            "src=[har du et landkort], target=[do you have a map], predicted=[do you have a map]\n",
            "src=[tom fortalte en vittighed], target=[tom told a joke], predicted=[tom told a joke]\n",
            "src=[hun begyndte at synge], target=[she began to sing], predicted=[she began to sing]\n",
            "src=[du ma gerne lane min guitar], target=[you may borrow my guitar], predicted=[you may borrow my guitar]\n",
            "src=[har du en bog], target=[do you have a book], predicted=[do you have a book]\n",
            "src=[det hele er toms skyld], target=[its all toms fault], predicted=[its all toms fault]\n",
            "src=[jeg er slet ikke sulten], target=[im not hungry at all], predicted=[im not hungry at all]\n",
            "src=[du er meget modig], target=[you are very courageous], predicted=[you are very courageous]\n",
            "src=[tom sov], target=[tom slept], predicted=[tom slept]\n",
            "src=[ma jeg tnde for fjernsynet], target=[can i turn on the tv], predicted=[can i turn on the tv]\n",
            "src=[hvordan fik du dem], target=[how did you get them], predicted=[how did you get them]\n",
            "src=[det er meget dyrt], target=[it is very expensive], predicted=[it is very expensive]\n",
            "src=[han er en frdig mand], target=[hes a goner], predicted=[hes a goner]\n",
            "src=[toms slips er gult], target=[toms tie is yellow], predicted=[toms tie is yellow]\n",
            "BLEU-1: 0.917646\n",
            "BLEU-2: 0.893623\n",
            "BLEU-3: 0.881702\n",
            "BLEU-4: 0.817984\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9fdWq0hluzV",
        "outputId": "22743fea-2992-4e87-d355-356b105a435a"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None\n",
        "\n",
        "# generate target given source sequence\n",
        "def predict_sequence(model, tokenizer, source):\n",
        "\tprediction = model.predict(source, verbose=0)[0]\n",
        "\tintegers = [argmax(vector) for vector in prediction]\n",
        "\ttarget = list()\n",
        "\tfor i in integers:\n",
        "\t\tword = word_for_id(i, tokenizer)\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\ttarget.append(word)\n",
        "\treturn ' '.join(target)\n",
        "\n",
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
        "\tactual, predicted = list(), list()\n",
        "\tfor i, source in enumerate(sources):\n",
        "\t\t# translate encoded source text\n",
        "\t\tsource = source.reshape((1, source.shape[0]))\n",
        "\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n",
        "\t\traw_target, raw_src,test = raw_dataset[i]\n",
        "\t\tif i < 100:\n",
        "\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
        "\t\tactual.append([raw_target.split()])\n",
        "\t\tpredicted.append(translation.split())\n",
        "\t# calculate BLEU score\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-danish-both.pkl')\n",
        "train = load_clean_sentences('english-danish-train.pkl')\n",
        "test = load_clean_sentences('english-danish-test.pkl')\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "# prepare data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "\n",
        "# load model\n",
        "model = load_model('model.h5')\n",
        "# test on some training sequences\n",
        "print('train')\n",
        "evaluate_model(model, eng_tokenizer, trainX, train)\n",
        "# test on some test sequences\n",
        "print('test')\n",
        "evaluate_model(model, eng_tokenizer, testX, test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train\n",
            "src=[kan du lide denne handtaske], target=[do you like this purse], predicted=[do you like this purse]\n",
            "src=[edderkopper spinder spind], target=[spiders spin webs], predicted=[spiders spin webs]\n",
            "src=[jeg haber at du har det godt], target=[i hope youre well], predicted=[i hope youre well]\n",
            "src=[vi er imod det], target=[we oppose that], predicted=[were oppose that]\n",
            "src=[vi er ikke rige], target=[were not rich], predicted=[were not rich]\n",
            "src=[jeg tager tit til stranden], target=[i often go to the beach], predicted=[i often go to the beach]\n",
            "src=[hvad er formalet med dem], target=[whats their purpose], predicted=[whats their purpose]\n",
            "src=[er hans navn virkelig tom], target=[is his name really tom], predicted=[is his name really tom]\n",
            "src=[det var en god historie], target=[it was a nice story], predicted=[it was a nice story]\n",
            "src=[tal for dig selv], target=[speak for yourself], predicted=[speak for yourself]\n",
            "src=[to plus to giver fire], target=[two plus two makes four], predicted=[two plus two makes four]\n",
            "src=[pink er ikke kun for piger], target=[pink isnt just for girls], predicted=[pink isnt just for girls]\n",
            "src=[du har fire hunde], target=[you have four dogs], predicted=[you have four dogs]\n",
            "src=[vis mig], target=[show me], predicted=[show me]\n",
            "src=[pas pa lommetyve], target=[beware of pickpockets], predicted=[beware of pickpockets pickpockets]\n",
            "src=[det er lige meget], target=[that doesnt matter], predicted=[it doesnt important]\n",
            "src=[tom blev fdt der], target=[tom was born there], predicted=[tom was born there]\n",
            "src=[hgen fangede en mus], target=[the hawk caught a mouse], predicted=[the hawk caught a mouse]\n",
            "src=[er din bil ny], target=[is your car new], predicted=[is your car new]\n",
            "src=[har i brug for et lift], target=[do you guys need a ride], predicted=[do you guys need a ride]\n",
            "src=[ingen blev saret], target=[nobody got hurt], predicted=[nobody got hurt]\n",
            "src=[jeg tager den billigste], target=[ill take the cheaper one], predicted=[ill take the cheaper one]\n",
            "src=[del og hersk], target=[divide and conquer], predicted=[divide and conquer]\n",
            "src=[kom sa snart du kan], target=[come as soon as possible], predicted=[come as soon as you can]\n",
            "src=[kan vi vente her], target=[can we wait here], predicted=[can we wait here]\n",
            "src=[du er uartig], target=[you are naughty], predicted=[you are naughty]\n",
            "src=[jeg tnker hele dagen pa tom], target=[i think about tom all day], predicted=[i think about tom all day]\n",
            "src=[tom vil til japan], target=[tom wants to go to japan], predicted=[tom wants to go to japan]\n",
            "src=[vr sa venlig at rkke mig saltet], target=[please pass me the salt], predicted=[please pass me the salt]\n",
            "src=[jeg kan lide kinesisk mad], target=[i like chinese food], predicted=[i like chinese food]\n",
            "src=[det glder mig at du bliver], target=[im glad youre staying], predicted=[im glad youre staying]\n",
            "src=[katten vagnede], target=[the cat woke up], predicted=[the cat woke up]\n",
            "src=[det er toms bil], target=[its toms car], predicted=[its toms car]\n",
            "src=[tom gav mig nogle bger], target=[tom gave me some books], predicted=[tom gave me some books]\n",
            "src=[krigen sluttede i], target=[the war ended in], predicted=[the war ended in]\n",
            "src=[kom nu ikke for sent i skole], target=[dont be late for school], predicted=[dont be late for school]\n",
            "src=[vask grntsagerne], target=[wash the vegetables], predicted=[wash the vegetables]\n",
            "src=[tag min], target=[take mine], predicted=[take mine]\n",
            "src=[det er ikke toms problem], target=[it isnt toms problem], predicted=[it isnt toms problem]\n",
            "src=[ingen kan hre mig], target=[nobody can hear me], predicted=[nobody can hear me]\n",
            "src=[mange brn gar med cowboybukser], target=[a lot of kids wear jeans], predicted=[a lot of kids wear jeans]\n",
            "src=[ring til mig i aften], target=[call me this evening], predicted=[call me this evening]\n",
            "src=[jeg vil hjlpe dig], target=[ill help you], predicted=[ill help to help]\n",
            "src=[hvad er der i skken], target=[whats in the bag], predicted=[whats in the bag]\n",
            "src=[har du kbt en ny bil], target=[did you buy a new car], predicted=[did you buy a new car]\n",
            "src=[han spiste hele blet], target=[he ate all of the apple], predicted=[he ate all of the apple]\n",
            "src=[tom ma have vret syg], target=[tom mustve been sick], predicted=[tom mustve been sick]\n",
            "src=[jeg har sovet pa baden], target=[i slept on the boat], predicted=[i slept on the boat]\n",
            "src=[hun spiser et ble], target=[shes eating an apple], predicted=[shes eating an apple]\n",
            "src=[lad os spille volleyball], target=[lets play volleyball], predicted=[lets play volleyball]\n",
            "src=[gid jeg var hjere], target=[i wish i were taller], predicted=[i wish i were taller]\n",
            "src=[vores hund er lbet vk], target=[our dog has gone away], predicted=[our dog has gone away]\n",
            "src=[hunden er dd], target=[the dog is dead], predicted=[the dog is dead]\n",
            "src=[tom havde lyst til at danse], target=[tom felt like dancing], predicted=[tom felt like dancing]\n",
            "src=[jeg har en], target=[i have one], predicted=[i have one]\n",
            "src=[tom laver ikke sjov], target=[tom isnt joking], predicted=[tom isnt joking]\n",
            "src=[jeg kan synge det pa engelsk], target=[i can sing it in english], predicted=[i can sing it in english]\n",
            "src=[tom hoster], target=[tom is coughing], predicted=[tom is coughing]\n",
            "src=[bare flg efter dem], target=[just follow them], predicted=[just follow them]\n",
            "src=[han er omtrent pa din alder], target=[he is about your age], predicted=[he is about your age]\n",
            "src=[jeg kender dig nsten ikke], target=[i barely know you], predicted=[i barely know you]\n",
            "src=[kortet er pa vggen], target=[the map is on the wall], predicted=[the map is on the wall]\n",
            "src=[tom kom hjem klokken fjorten tredive], target=[tom arrived home at], predicted=[tom arrived home at]\n",
            "src=[mistnker de mig], target=[do they suspect me], predicted=[do they suspect me]\n",
            "src=[i er dovne], target=[youre lazy], predicted=[youre lazy]\n",
            "src=[er det blat], target=[is it blue], predicted=[is that blue]\n",
            "src=[hvem har brug for en drink], target=[who needs a drink], predicted=[who needs a drink]\n",
            "src=[tom blev kidnappet], target=[tom got kidnapped], predicted=[tom got kidnapped]\n",
            "src=[hvor er hans familie], target=[where is his family], predicted=[where is his family]\n",
            "src=[du er gammel], target=[youre old], predicted=[youre old]\n",
            "src=[hun er slave af mode], target=[she is a slave of fashion], predicted=[she is a slave of fashion]\n",
            "src=[er de her blyanter dine], target=[are these pencils yours], predicted=[are these pencils yours]\n",
            "src=[tom er god til at lave mad], target=[tom is good at cooking], predicted=[tom is good at cooking]\n",
            "src=[jeg driller dig bare tom], target=[im just teasing you tom], predicted=[im just teasing you tom]\n",
            "src=[hvad er din profession], target=[whats your occupation], predicted=[whats your occupation]\n",
            "src=[jeg er et menneske], target=[im a human being], predicted=[im a human being]\n",
            "src=[du har ikke noget valg], target=[you have no choice], predicted=[you have no choice]\n",
            "src=[tom arbejder i en bank], target=[tom works at a bank], predicted=[tom works in a bank]\n",
            "src=[sover tom stadig], target=[is tom still asleep], predicted=[is tom still asleep]\n",
            "src=[far jeg hvad jeg nsker], target=[will i get what i want], predicted=[will i get what i want]\n",
            "src=[den hund sprang], target=[that dog jumped], predicted=[that dog jumped]\n",
            "src=[bed mig ikke om penge], target=[dont ask me for money], predicted=[dont ask me for money]\n",
            "src=[jeg har dobbelt statsborgerskab], target=[i have dual citizenship], predicted=[i have dual citizenship]\n",
            "src=[nogen stjal min rygsk], target=[someone stole my rucksack], predicted=[someone stole my rucksack]\n",
            "src=[jeg er ikke din ven], target=[im not your friend], predicted=[im not your friend]\n",
            "src=[jeg kan vinde denne gang], target=[i can win this time], predicted=[i can win this time]\n",
            "src=[du ramte mig nsten], target=[you almost hit me], predicted=[you almost hit me]\n",
            "src=[dette ur er vandtt], target=[this watch is waterproof], predicted=[this watch is waterproof]\n",
            "src=[fyrre personer var til stede], target=[forty people were present], predicted=[forty people were present]\n",
            "src=[vi har skrevet til dem], target=[weve written to them], predicted=[weve written to them]\n",
            "src=[skriv med pen], target=[write with a pen], predicted=[write with a pen]\n",
            "src=[vi drak l], target=[we drank beer], predicted=[we drank beer]\n",
            "src=[vores tog er forsinket], target=[our train is delayed], predicted=[our train is late]\n",
            "src=[smr er lavet af mlk], target=[butter is made from milk], predicted=[butter is made from milk]\n",
            "src=[tom har tre katte], target=[tom has three cats], predicted=[tom has three cats]\n",
            "src=[han bor dr om dr med os], target=[he lives next door to us], predicted=[he lives next door to us]\n",
            "src=[jeg tror pa dig], target=[i do believe you], predicted=[i believe in you]\n",
            "src=[jeg er enig], target=[i agree with you], predicted=[i agree]\n",
            "src=[hvem har lavet snemanden], target=[who built the snowman], predicted=[who built the snowman]\n",
            "src=[jeg kan ikke vente lngere], target=[i cant wait any more], predicted=[i cant wait any more]\n",
            "BLEU-1: 0.962770\n",
            "BLEU-2: 0.948699\n",
            "BLEU-3: 0.937729\n",
            "BLEU-4: 0.880831\n",
            "test\n",
            "src=[jeg er ikke bange], target=[i am not afraid], predicted=[i am not afraid]\n",
            "src=[tom er ved at lse bibelen], target=[tom is reading the bible], predicted=[tom is reading the bible]\n",
            "src=[jeg har allerede tjekket ud], target=[ive already checked out], predicted=[ive already checked out]\n",
            "src=[tom dde af tyfus], target=[tom died of typhus], predicted=[tom died of typhus]\n",
            "src=[det er ikke hvad jeg bestilte], target=[it is not what i ordered], predicted=[it is not what i ordered]\n",
            "src=[jeg haber virkelig du har ret], target=[i do hope youre right], predicted=[i do hope youre right]\n",
            "src=[jeg er ikke en robot], target=[im not a robot], predicted=[im not a robot]\n",
            "src=[det er ikke besvret vrd], target=[its not worth the effort], predicted=[it isnt worth the effort]\n",
            "src=[er det et ja eller et nej], target=[is that a yes or no], predicted=[is that a yes or a no]\n",
            "src=[blse vre med det], target=[never mind], predicted=[never mind]\n",
            "src=[jeg har vret vk for lnge], target=[ive stayed away too long], predicted=[ive stayed away too long]\n",
            "src=[jeg har mistet mit ur], target=[i have lost my watch], predicted=[i have lost my watch]\n",
            "src=[tnk over det], target=[think about it], predicted=[think about it]\n",
            "src=[tom er i bilen], target=[tom is in the car], predicted=[tom is in the car]\n",
            "src=[tom grd hele natten], target=[tom cried all night], predicted=[tom cried all night]\n",
            "src=[hvad er krlighed], target=[what is love], predicted=[what is love]\n",
            "src=[sa du min far], target=[did you see my father], predicted=[did you see my father]\n",
            "src=[har du brn], target=[do you have any kids], predicted=[do you have any children]\n",
            "src=[jeg er bankerot], target=[im broke], predicted=[im broke]\n",
            "src=[hvor var du], target=[where were you], predicted=[where were you]\n",
            "src=[du ma have en god tur], target=[have a nice trip], predicted=[have a nice trip]\n",
            "src=[tom arbejder for meget], target=[tom works too much], predicted=[tom works too much]\n",
            "src=[jeg er student], target=[im a student], predicted=[i am student student]\n",
            "src=[tom mistede sin teddybjrn], target=[tom lost his teddy bear], predicted=[tom lost his teddy bear]\n",
            "src=[tom tag disse handsker pa], target=[tom put these gloves on], predicted=[tom put these gloves on]\n",
            "src=[det er en direkte ordre], target=[thats a direct order], predicted=[thats a direct order]\n",
            "src=[tom holder et sammenskudsgilde], target=[tom is having a potluck], predicted=[tom is having a potluck]\n",
            "src=[vi kan lide ham], target=[we like him], predicted=[we like him]\n",
            "src=[tom arbejder], target=[tom is working], predicted=[tom works]\n",
            "src=[kbte i en ny bil], target=[did you buy a new car], predicted=[did you buy a new car]\n",
            "src=[jeg gav penge til alle], target=[i gave money to everyone], predicted=[i gave money to everyone]\n",
            "src=[hvad lavede du om natten], target=[what did you do at night], predicted=[what did you do at night]\n",
            "src=[dette er et smukt tr], target=[this is a beautiful tree], predicted=[this is a beautiful tree]\n",
            "src=[du kan slette det nu], target=[you can delete that now], predicted=[you can delete that now]\n",
            "src=[det er for sent for mig], target=[its too late for me], predicted=[it is too for for me]\n",
            "src=[er den stadig til salg], target=[is it still for sale], predicted=[is it still for sale]\n",
            "src=[hvad er problemet], target=[whats the problem], predicted=[whats the problem]\n",
            "src=[tom er et almindeligt navn], target=[tom is a common name], predicted=[tom is a common name]\n",
            "src=[drej til venstre], target=[turn left], predicted=[turn to the]\n",
            "src=[ildebrand], target=[fire], predicted=[fire]\n",
            "src=[de sad i en cirkel], target=[they sat in a circle], predicted=[they sat in a circle]\n",
            "src=[kan du pakke den ind som en gave], target=[can you gift wrap that], predicted=[can you gift wrap that]\n",
            "src=[er det din], target=[is it yours], predicted=[is it yours]\n",
            "src=[dette er et ble], target=[this is an apple], predicted=[this is an apple]\n",
            "src=[du er canadisk ikke sandt], target=[youre canadian right], predicted=[youre canadian right]\n",
            "src=[jeg br gre rent], target=[i should clean up], predicted=[i should clean up]\n",
            "src=[vi er sskendebrn], target=[we are cousins], predicted=[were cousins cousins]\n",
            "src=[hvilken er bedst], target=[which is better], predicted=[which is better]\n",
            "src=[jeg kom tomhndet hjem], target=[i came home empty handed], predicted=[i came home empty handed]\n",
            "src=[han vendte tilbage fra amerika], target=[he came back from america], predicted=[he came back from america]\n",
            "src=[jeg tror de sa os], target=[i think they saw us], predicted=[i think they saw us]\n",
            "src=[tom har mavepine], target=[tom has a stomachache], predicted=[tom has a stomachache]\n",
            "src=[hun er hj og smuk], target=[shes tall and beautiful], predicted=[shes tall and beautiful]\n",
            "src=[jeg tror at du har ret], target=[i think that youre right], predicted=[i think that youre right]\n",
            "src=[jeg sa tom d], target=[i saw tom die], predicted=[i saw tom die]\n",
            "src=[jeg er ikke skaldet], target=[im not bald], predicted=[im not bald]\n",
            "src=[tom sagde at du var kommet], target=[tom said that youd come], predicted=[tom said that youd come]\n",
            "src=[hvordan har du det], target=[how are you], predicted=[how do you do]\n",
            "src=[jeg kan ikke lide navnet tom], target=[i dont like the name tom], predicted=[i dont like the name tom]\n",
            "src=[katten jagede musen], target=[the cat chased the mouse], predicted=[the cat chased the mouse]\n",
            "src=[rdvin tak], target=[red wine please], predicted=[red wine please]\n",
            "src=[jeg bryder mig ikke om lektier], target=[i dont like homework], predicted=[i dont like homework]\n",
            "src=[vi er mellem venner], target=[were among friends], predicted=[were among friends]\n",
            "src=[mary er ikke srlig feminin], target=[mary isnt very feminine], predicted=[mary isnt very feminine]\n",
            "src=[lad mig hjlpe til], target=[let me help], predicted=[let me help]\n",
            "src=[tom blev pakrt af en lastbil], target=[tom got hit by a truck], predicted=[tom got hit by a truck]\n",
            "src=[glo ikke pa mig], target=[dont stare at me], predicted=[dont stare at me]\n",
            "src=[jeg har kun en hund], target=[i only have one dog], predicted=[i only have one dog]\n",
            "src=[vi vil hjlpe dig], target=[well help you], predicted=[well help you]\n",
            "src=[det hagler], target=[its hailing], predicted=[its hailing]\n",
            "src=[jeg arbejder for dette firma], target=[i work for this company], predicted=[i work for this company]\n",
            "src=[det kan du ndre], target=[you can change that], predicted=[you can change that]\n",
            "src=[min mor er altid optaget], target=[my mother is always busy], predicted=[my mother is always busy]\n",
            "src=[jeg savner virkelig min mand], target=[i really miss my husband], predicted=[i really miss my husband]\n",
            "src=[luk venligst vinduet], target=[please close the window], predicted=[please close the window]\n",
            "src=[jeg troede at du havde ret], target=[i thought you were right], predicted=[i thought you were right]\n",
            "src=[jeg tror det kunne vre en softwarefejl], target=[i think it might be a bug], predicted=[i think it might be a bug]\n",
            "src=[ladeporten var aben], target=[the barn door was open], predicted=[the barn door was open]\n",
            "src=[det stinker af helvede til herinde], target=[it really stinks in here], predicted=[it really stinks in here]\n",
            "src=[er din kone hj], target=[is your wife tall], predicted=[is your wife tall]\n",
            "src=[tom har bestilt roomservice], target=[tom ordered room service], predicted=[tom ordered room service]\n",
            "src=[skriv dit navn fuldt ud], target=[write your name in full], predicted=[write your name in full]\n",
            "src=[er du lge], target=[are you a doctor], predicted=[are you a doctor]\n",
            "src=[pas pa], target=[be careful], predicted=[be careful]\n",
            "src=[mary slog op med mig], target=[mary broke up with me], predicted=[mary broke up with me]\n",
            "src=[sknhed er subjektivt], target=[beauty is subjective], predicted=[beauty is subjective]\n",
            "src=[tom er en svkling], target=[tom is a wimp], predicted=[tom is a wimp]\n",
            "src=[hvor har tom vret], target=[where has tom been], predicted=[wheres has been been]\n",
            "src=[tom kan cykle], target=[tom can ride a bicycle], predicted=[tom can ride a bicycle]\n",
            "src=[tom sagde at du ville komme], target=[tom said that youd come], predicted=[tom said that would come]\n",
            "src=[hvad er det du planlgger], target=[what are you planning], predicted=[what are you planning]\n",
            "src=[jeg nsker at forklare], target=[i want to explain], predicted=[i want to explain]\n",
            "src=[spdbarnet ligger og sover], target=[the babys sleeping], predicted=[the baby is sleeping]\n",
            "src=[tom kan ikke ga], target=[tom cant walk], predicted=[tom cant walk]\n",
            "src=[jeg tror at hans navn er tom], target=[i believe his name is tom], predicted=[i believe his name is tom]\n",
            "src=[jeg tager af sted nu], target=[im taking off now], predicted=[im taking off now]\n",
            "src=[tom var ved at d af trst], target=[tom was dying of thirst], predicted=[tom was dying of thirst]\n",
            "src=[penge er magt], target=[money is power], predicted=[money is power]\n",
            "src=[denne appelsin er velsmagende], target=[this orange is delicious], predicted=[this orange is delicious]\n",
            "src=[jeg har ikke drukket den mlk], target=[i didnt drink that milk], predicted=[i didnt drink that milk]\n",
            "BLEU-1: 0.918388\n",
            "BLEU-2: 0.894117\n",
            "BLEU-3: 0.882622\n",
            "BLEU-4: 0.818820\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ya7o7JRJNyDT",
        "outputId": "ffd1117e-3e41-4876-e0ca-1e967a5ef608"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# one hot encode target sequence\n",
        "def encode_output(sequences, vocab_size):\n",
        "\tylist = list()\n",
        "\tfor sequence in sequences:\n",
        "\t\tencoded = to_categorical(sequence, num_classes=vocab_size)\n",
        "\t\tylist.append(encoded)\n",
        "\ty = array(ylist)\n",
        "\ty = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
        "\treturn y\n",
        "\n",
        "# define NMT model\n",
        "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
        "\tmodel.add(LSTM(n_units))\n",
        "\tmodel.add(RepeatVector(tar_timesteps))\n",
        "\tmodel.add(LSTM(n_units, return_sequences=True))\n",
        "\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
        "\treturn model\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-danish-both.pkl')\n",
        "train = load_clean_sentences('english-danish-train.pkl')\n",
        "test = load_clean_sentences('english-danish-test.pkl')\n",
        "\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
        "print('English Max Length: %d' % (eng_length))\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "print('Danish Vocabulary Size: %d' % ger_vocab_size)\n",
        "print('Danish Max Length: %d' % (ger_length))\n",
        "\n",
        "# prepare training data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
        "trainY = encode_output(trainY, eng_vocab_size)\n",
        "# prepare validation data\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
        "testY = encode_output(testY, eng_vocab_size)\n",
        "\n",
        "# define model\n",
        "model = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 256)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "# summarize defined model\n",
        "print(model.summary())\n",
        "plot_model(model, to_file='model.png', show_shapes=True)\n",
        "# fit model\n",
        "filename = 'model.h5'\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "model.fit(trainX, trainY, epochs=100, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English Vocabulary Size: 3297\n",
            "English Max Length: 8\n",
            "Danish Vocabulary Size: 4246\n",
            "Danish Max Length: 11\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 11, 256)           1086976   \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 256)               525312    \n",
            "_________________________________________________________________\n",
            "repeat_vector_1 (RepeatVecto (None, 8, 256)            0         \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 8, 256)            525312    \n",
            "_________________________________________________________________\n",
            "time_distributed_1 (TimeDist (None, 8, 3297)           847329    \n",
            "=================================================================\n",
            "Total params: 2,984,929\n",
            "Trainable params: 2,984,929\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "141/141 - 51s - loss: 4.0321 - val_loss: 3.2508\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 3.25084, saving model to model.h5\n",
            "Epoch 2/100\n",
            "141/141 - 45s - loss: 3.1788 - val_loss: 3.0743\n",
            "\n",
            "Epoch 00002: val_loss improved from 3.25084 to 3.07429, saving model to model.h5\n",
            "Epoch 3/100\n",
            "141/141 - 44s - loss: 3.0667 - val_loss: 3.0046\n",
            "\n",
            "Epoch 00003: val_loss improved from 3.07429 to 3.00456, saving model to model.h5\n",
            "Epoch 4/100\n",
            "141/141 - 45s - loss: 2.9971 - val_loss: 2.9334\n",
            "\n",
            "Epoch 00004: val_loss improved from 3.00456 to 2.93342, saving model to model.h5\n",
            "Epoch 5/100\n",
            "141/141 - 44s - loss: 2.9315 - val_loss: 2.8748\n",
            "\n",
            "Epoch 00005: val_loss improved from 2.93342 to 2.87482, saving model to model.h5\n",
            "Epoch 6/100\n",
            "141/141 - 45s - loss: 2.8545 - val_loss: 2.7783\n",
            "\n",
            "Epoch 00006: val_loss improved from 2.87482 to 2.77834, saving model to model.h5\n",
            "Epoch 7/100\n",
            "141/141 - 45s - loss: 2.7574 - val_loss: 2.6859\n",
            "\n",
            "Epoch 00007: val_loss improved from 2.77834 to 2.68587, saving model to model.h5\n",
            "Epoch 8/100\n",
            "141/141 - 44s - loss: 2.6660 - val_loss: 2.6005\n",
            "\n",
            "Epoch 00008: val_loss improved from 2.68587 to 2.60049, saving model to model.h5\n",
            "Epoch 9/100\n",
            "141/141 - 46s - loss: 2.5725 - val_loss: 2.4957\n",
            "\n",
            "Epoch 00009: val_loss improved from 2.60049 to 2.49567, saving model to model.h5\n",
            "Epoch 10/100\n",
            "141/141 - 45s - loss: 2.4602 - val_loss: 2.3854\n",
            "\n",
            "Epoch 00010: val_loss improved from 2.49567 to 2.38542, saving model to model.h5\n",
            "Epoch 11/100\n",
            "141/141 - 44s - loss: 2.3409 - val_loss: 2.2609\n",
            "\n",
            "Epoch 00011: val_loss improved from 2.38542 to 2.26086, saving model to model.h5\n",
            "Epoch 12/100\n",
            "141/141 - 45s - loss: 2.2174 - val_loss: 2.1508\n",
            "\n",
            "Epoch 00012: val_loss improved from 2.26086 to 2.15077, saving model to model.h5\n",
            "Epoch 13/100\n",
            "141/141 - 45s - loss: 2.1028 - val_loss: 2.0414\n",
            "\n",
            "Epoch 00013: val_loss improved from 2.15077 to 2.04138, saving model to model.h5\n",
            "Epoch 14/100\n",
            "141/141 - 45s - loss: 1.9906 - val_loss: 1.9306\n",
            "\n",
            "Epoch 00014: val_loss improved from 2.04138 to 1.93057, saving model to model.h5\n",
            "Epoch 15/100\n",
            "141/141 - 45s - loss: 1.8869 - val_loss: 1.8341\n",
            "\n",
            "Epoch 00015: val_loss improved from 1.93057 to 1.83409, saving model to model.h5\n",
            "Epoch 16/100\n",
            "141/141 - 46s - loss: 1.7805 - val_loss: 1.7419\n",
            "\n",
            "Epoch 00016: val_loss improved from 1.83409 to 1.74185, saving model to model.h5\n",
            "Epoch 17/100\n",
            "141/141 - 45s - loss: 1.6836 - val_loss: 1.6591\n",
            "\n",
            "Epoch 00017: val_loss improved from 1.74185 to 1.65905, saving model to model.h5\n",
            "Epoch 18/100\n",
            "141/141 - 44s - loss: 1.5901 - val_loss: 1.5688\n",
            "\n",
            "Epoch 00018: val_loss improved from 1.65905 to 1.56875, saving model to model.h5\n",
            "Epoch 19/100\n",
            "141/141 - 45s - loss: 1.4981 - val_loss: 1.4945\n",
            "\n",
            "Epoch 00019: val_loss improved from 1.56875 to 1.49448, saving model to model.h5\n",
            "Epoch 20/100\n",
            "141/141 - 45s - loss: 1.4092 - val_loss: 1.3987\n",
            "\n",
            "Epoch 00020: val_loss improved from 1.49448 to 1.39871, saving model to model.h5\n",
            "Epoch 21/100\n",
            "141/141 - 45s - loss: 1.3183 - val_loss: 1.3209\n",
            "\n",
            "Epoch 00021: val_loss improved from 1.39871 to 1.32088, saving model to model.h5\n",
            "Epoch 22/100\n",
            "141/141 - 45s - loss: 1.2403 - val_loss: 1.2470\n",
            "\n",
            "Epoch 00022: val_loss improved from 1.32088 to 1.24695, saving model to model.h5\n",
            "Epoch 23/100\n",
            "141/141 - 45s - loss: 1.1582 - val_loss: 1.1803\n",
            "\n",
            "Epoch 00023: val_loss improved from 1.24695 to 1.18030, saving model to model.h5\n",
            "Epoch 24/100\n",
            "141/141 - 44s - loss: 1.0832 - val_loss: 1.1114\n",
            "\n",
            "Epoch 00024: val_loss improved from 1.18030 to 1.11143, saving model to model.h5\n",
            "Epoch 25/100\n",
            "141/141 - 45s - loss: 1.0093 - val_loss: 1.0385\n",
            "\n",
            "Epoch 00025: val_loss improved from 1.11143 to 1.03848, saving model to model.h5\n",
            "Epoch 26/100\n",
            "141/141 - 45s - loss: 0.9377 - val_loss: 0.9817\n",
            "\n",
            "Epoch 00026: val_loss improved from 1.03848 to 0.98170, saving model to model.h5\n",
            "Epoch 27/100\n",
            "141/141 - 45s - loss: 0.8731 - val_loss: 0.9249\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.98170 to 0.92494, saving model to model.h5\n",
            "Epoch 28/100\n",
            "141/141 - 44s - loss: 0.8110 - val_loss: 0.8791\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.92494 to 0.87905, saving model to model.h5\n",
            "Epoch 29/100\n",
            "141/141 - 45s - loss: 0.7541 - val_loss: 0.8219\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.87905 to 0.82189, saving model to model.h5\n",
            "Epoch 30/100\n",
            "141/141 - 45s - loss: 0.6962 - val_loss: 0.7836\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.82189 to 0.78361, saving model to model.h5\n",
            "Epoch 31/100\n",
            "141/141 - 44s - loss: 0.6489 - val_loss: 0.7358\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.78361 to 0.73581, saving model to model.h5\n",
            "Epoch 32/100\n",
            "141/141 - 44s - loss: 0.6030 - val_loss: 0.6993\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.73581 to 0.69930, saving model to model.h5\n",
            "Epoch 33/100\n",
            "141/141 - 44s - loss: 0.5556 - val_loss: 0.6589\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.69930 to 0.65890, saving model to model.h5\n",
            "Epoch 34/100\n",
            "141/141 - 45s - loss: 0.5141 - val_loss: 0.6262\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.65890 to 0.62624, saving model to model.h5\n",
            "Epoch 35/100\n",
            "141/141 - 45s - loss: 0.4780 - val_loss: 0.6025\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.62624 to 0.60247, saving model to model.h5\n",
            "Epoch 36/100\n",
            "141/141 - 45s - loss: 0.4462 - val_loss: 0.5754\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.60247 to 0.57538, saving model to model.h5\n",
            "Epoch 37/100\n",
            "141/141 - 45s - loss: 0.4102 - val_loss: 0.5482\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.57538 to 0.54818, saving model to model.h5\n",
            "Epoch 38/100\n",
            "141/141 - 45s - loss: 0.3803 - val_loss: 0.5184\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.54818 to 0.51839, saving model to model.h5\n",
            "Epoch 39/100\n",
            "141/141 - 45s - loss: 0.3537 - val_loss: 0.4992\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.51839 to 0.49924, saving model to model.h5\n",
            "Epoch 40/100\n",
            "141/141 - 48s - loss: 0.3268 - val_loss: 0.4778\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.49924 to 0.47778, saving model to model.h5\n",
            "Epoch 41/100\n",
            "141/141 - 45s - loss: 0.3055 - val_loss: 0.4609\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.47778 to 0.46092, saving model to model.h5\n",
            "Epoch 42/100\n",
            "141/141 - 44s - loss: 0.2827 - val_loss: 0.4445\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.46092 to 0.44447, saving model to model.h5\n",
            "Epoch 43/100\n",
            "141/141 - 45s - loss: 0.2645 - val_loss: 0.4315\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.44447 to 0.43146, saving model to model.h5\n",
            "Epoch 44/100\n",
            "141/141 - 44s - loss: 0.2458 - val_loss: 0.4168\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.43146 to 0.41682, saving model to model.h5\n",
            "Epoch 45/100\n",
            "141/141 - 45s - loss: 0.2333 - val_loss: 0.4050\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.41682 to 0.40501, saving model to model.h5\n",
            "Epoch 46/100\n",
            "141/141 - 45s - loss: 0.2146 - val_loss: 0.3909\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.40501 to 0.39094, saving model to model.h5\n",
            "Epoch 47/100\n",
            "141/141 - 45s - loss: 0.1997 - val_loss: 0.3869\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.39094 to 0.38691, saving model to model.h5\n",
            "Epoch 48/100\n",
            "141/141 - 45s - loss: 0.1893 - val_loss: 0.3741\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.38691 to 0.37408, saving model to model.h5\n",
            "Epoch 49/100\n",
            "141/141 - 45s - loss: 0.1761 - val_loss: 0.3677\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.37408 to 0.36772, saving model to model.h5\n",
            "Epoch 50/100\n",
            "141/141 - 45s - loss: 0.1661 - val_loss: 0.3603\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.36772 to 0.36031, saving model to model.h5\n",
            "Epoch 51/100\n",
            "141/141 - 45s - loss: 0.1580 - val_loss: 0.3530\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.36031 to 0.35298, saving model to model.h5\n",
            "Epoch 52/100\n",
            "141/141 - 44s - loss: 0.1520 - val_loss: 0.3491\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.35298 to 0.34910, saving model to model.h5\n",
            "Epoch 53/100\n",
            "141/141 - 46s - loss: 0.1418 - val_loss: 0.3431\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.34910 to 0.34315, saving model to model.h5\n",
            "Epoch 54/100\n",
            "141/141 - 44s - loss: 0.1348 - val_loss: 0.3396\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.34315 to 0.33959, saving model to model.h5\n",
            "Epoch 55/100\n",
            "141/141 - 46s - loss: 0.1269 - val_loss: 0.3321\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.33959 to 0.33207, saving model to model.h5\n",
            "Epoch 56/100\n",
            "141/141 - 44s - loss: 0.1204 - val_loss: 0.3286\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.33207 to 0.32862, saving model to model.h5\n",
            "Epoch 57/100\n",
            "141/141 - 45s - loss: 0.1150 - val_loss: 0.3262\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.32862 to 0.32616, saving model to model.h5\n",
            "Epoch 58/100\n",
            "141/141 - 45s - loss: 0.1100 - val_loss: 0.3233\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.32616 to 0.32334, saving model to model.h5\n",
            "Epoch 59/100\n",
            "141/141 - 45s - loss: 0.1059 - val_loss: 0.3190\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.32334 to 0.31901, saving model to model.h5\n",
            "Epoch 60/100\n",
            "141/141 - 44s - loss: 0.1019 - val_loss: 0.3180\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.31901 to 0.31803, saving model to model.h5\n",
            "Epoch 61/100\n",
            "141/141 - 44s - loss: 0.1002 - val_loss: 0.3216\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.31803\n",
            "Epoch 62/100\n",
            "141/141 - 44s - loss: 0.0990 - val_loss: 0.3170\n",
            "\n",
            "Epoch 00062: val_loss improved from 0.31803 to 0.31695, saving model to model.h5\n",
            "Epoch 63/100\n",
            "141/141 - 44s - loss: 0.0954 - val_loss: 0.3133\n",
            "\n",
            "Epoch 00063: val_loss improved from 0.31695 to 0.31334, saving model to model.h5\n",
            "Epoch 64/100\n",
            "141/141 - 44s - loss: 0.0911 - val_loss: 0.3116\n",
            "\n",
            "Epoch 00064: val_loss improved from 0.31334 to 0.31158, saving model to model.h5\n",
            "Epoch 65/100\n",
            "141/141 - 44s - loss: 0.0881 - val_loss: 0.3112\n",
            "\n",
            "Epoch 00065: val_loss improved from 0.31158 to 0.31125, saving model to model.h5\n",
            "Epoch 66/100\n",
            "141/141 - 44s - loss: 0.0857 - val_loss: 0.3088\n",
            "\n",
            "Epoch 00066: val_loss improved from 0.31125 to 0.30883, saving model to model.h5\n",
            "Epoch 67/100\n",
            "141/141 - 44s - loss: 0.0845 - val_loss: 0.3082\n",
            "\n",
            "Epoch 00067: val_loss improved from 0.30883 to 0.30820, saving model to model.h5\n",
            "Epoch 68/100\n",
            "141/141 - 44s - loss: 0.0820 - val_loss: 0.3118\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.30820\n",
            "Epoch 69/100\n",
            "141/141 - 44s - loss: 0.0821 - val_loss: 0.3084\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.30820\n",
            "Epoch 70/100\n",
            "141/141 - 44s - loss: 0.0797 - val_loss: 0.3073\n",
            "\n",
            "Epoch 00070: val_loss improved from 0.30820 to 0.30733, saving model to model.h5\n",
            "Epoch 71/100\n",
            "141/141 - 45s - loss: 0.0786 - val_loss: 0.3084\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.30733\n",
            "Epoch 72/100\n",
            "141/141 - 44s - loss: 0.0786 - val_loss: 0.3096\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.30733\n",
            "Epoch 73/100\n",
            "141/141 - 44s - loss: 0.0801 - val_loss: 0.3100\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.30733\n",
            "Epoch 74/100\n",
            "141/141 - 44s - loss: 0.0779 - val_loss: 0.3076\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.30733\n",
            "Epoch 75/100\n",
            "141/141 - 44s - loss: 0.0757 - val_loss: 0.3066\n",
            "\n",
            "Epoch 00075: val_loss improved from 0.30733 to 0.30661, saving model to model.h5\n",
            "Epoch 76/100\n",
            "141/141 - 44s - loss: 0.0717 - val_loss: 0.3054\n",
            "\n",
            "Epoch 00076: val_loss improved from 0.30661 to 0.30538, saving model to model.h5\n",
            "Epoch 77/100\n",
            "141/141 - 44s - loss: 0.0695 - val_loss: 0.3062\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.30538\n",
            "Epoch 78/100\n",
            "141/141 - 45s - loss: 0.0689 - val_loss: 0.3051\n",
            "\n",
            "Epoch 00078: val_loss improved from 0.30538 to 0.30513, saving model to model.h5\n",
            "Epoch 79/100\n",
            "141/141 - 45s - loss: 0.0687 - val_loss: 0.3060\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.30513\n",
            "Epoch 80/100\n",
            "141/141 - 45s - loss: 0.0690 - val_loss: 0.3051\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.30513\n",
            "Epoch 81/100\n",
            "141/141 - 44s - loss: 0.0678 - val_loss: 0.3055\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.30513\n",
            "Epoch 82/100\n",
            "141/141 - 44s - loss: 0.0666 - val_loss: 0.3047\n",
            "\n",
            "Epoch 00082: val_loss improved from 0.30513 to 0.30471, saving model to model.h5\n",
            "Epoch 83/100\n",
            "141/141 - 44s - loss: 0.0665 - val_loss: 0.3054\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.30471\n",
            "Epoch 84/100\n",
            "141/141 - 44s - loss: 0.0752 - val_loss: 0.3211\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.30471\n",
            "Epoch 85/100\n",
            "141/141 - 44s - loss: 0.0880 - val_loss: 0.3243\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.30471\n",
            "Epoch 86/100\n",
            "141/141 - 44s - loss: 0.0846 - val_loss: 0.3188\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.30471\n",
            "Epoch 87/100\n",
            "141/141 - 44s - loss: 0.0751 - val_loss: 0.3099\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.30471\n",
            "Epoch 88/100\n",
            "141/141 - 45s - loss: 0.0676 - val_loss: 0.3064\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.30471\n",
            "Epoch 89/100\n",
            "141/141 - 44s - loss: 0.0639 - val_loss: 0.3048\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.30471\n",
            "Epoch 90/100\n",
            "141/141 - 44s - loss: 0.0618 - val_loss: 0.3045\n",
            "\n",
            "Epoch 00090: val_loss improved from 0.30471 to 0.30446, saving model to model.h5\n",
            "Epoch 91/100\n",
            "141/141 - 45s - loss: 0.0599 - val_loss: 0.3026\n",
            "\n",
            "Epoch 00091: val_loss improved from 0.30446 to 0.30260, saving model to model.h5\n",
            "Epoch 92/100\n",
            "141/141 - 44s - loss: 0.0583 - val_loss: 0.3050\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.30260\n",
            "Epoch 93/100\n",
            "141/141 - 44s - loss: 0.0585 - val_loss: 0.3042\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.30260\n",
            "Epoch 94/100\n",
            "141/141 - 44s - loss: 0.0589 - val_loss: 0.3054\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.30260\n",
            "Epoch 95/100\n",
            "141/141 - 45s - loss: 0.0585 - val_loss: 0.3050\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.30260\n",
            "Epoch 96/100\n",
            "141/141 - 45s - loss: 0.0582 - val_loss: 0.3057\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.30260\n",
            "Epoch 97/100\n",
            "141/141 - 45s - loss: 0.0583 - val_loss: 0.3064\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.30260\n",
            "Epoch 98/100\n",
            "141/141 - 44s - loss: 0.0581 - val_loss: 0.3078\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.30260\n",
            "Epoch 99/100\n",
            "141/141 - 44s - loss: 0.0595 - val_loss: 0.3108\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.30260\n",
            "Epoch 100/100\n",
            "141/141 - 45s - loss: 0.0632 - val_loss: 0.3137\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.30260\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f251ce156d8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWmittHKEwu5",
        "outputId": "7563d2ca-58e7-4296-b57d-c9ef394f60f0"
      },
      "source": [
        "##30 epoch\n",
        "from pickle import load\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None\n",
        "\n",
        "# generate target given source sequence\n",
        "def predict_sequence(model, tokenizer, source):\n",
        "\tprediction = model.predict(source, verbose=0)[0]\n",
        "\tintegers = [argmax(vector) for vector in prediction]\n",
        "\ttarget = list()\n",
        "\tfor i in integers:\n",
        "\t\tword = word_for_id(i, tokenizer)\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\ttarget.append(word)\n",
        "\treturn ' '.join(target)\n",
        "\n",
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
        "\tactual, predicted = list(), list()\n",
        "\tfor i, source in enumerate(sources):\n",
        "\t\t# translate encoded source text\n",
        "\t\tsource = source.reshape((1, source.shape[0]))\n",
        "\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n",
        "\t\traw_target, raw_src,test = raw_dataset[i]\n",
        "\t\tif i < 10:\n",
        "\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
        "\t\tactual.append([raw_target.split()])\n",
        "\t\tpredicted.append(translation.split())\n",
        "\t# calculate BLEU score\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-danish-both.pkl')\n",
        "train = load_clean_sentences('english-danish-train.pkl')\n",
        "test = load_clean_sentences('english-danish-test.pkl')\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "# prepare data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "\n",
        "# load model\n",
        "model = load_model('model.h5')\n",
        "# test on some training sequences\n",
        "print('train')\n",
        "evaluate_model(model, eng_tokenizer, trainX, train)\n",
        "# test on some test sequences\n",
        "print('test')\n",
        "evaluate_model(model, eng_tokenizer, testX, test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train\n",
            "src=[hvad lavede du om aftenen], target=[what did you do at night], predicted=[what did you do at night]\n",
            "src=[er det alt], target=[is that it], predicted=[is it everything]\n",
            "src=[jeg har set en ufo], target=[i have seen a ufo], predicted=[i have seen a beer]\n",
            "src=[hun var deprimeret], target=[she felt blue], predicted=[she felt blue]\n",
            "src=[jeg er bedre], target=[i am better], predicted=[i am better]\n",
            "src=[jeg tror tom kan lide mary], target=[i think tom likes mary], predicted=[i think tom tom tom]\n",
            "src=[jeg vil fortlle dig sandheden], target=[ill tell you the truth], predicted=[ill get tom the]\n",
            "src=[tom arbejder], target=[tom is working], predicted=[tom is]\n",
            "src=[aftensmaden er klar], target=[dinner is ready to eat], predicted=[dinners is ready]\n",
            "src=[noget ma der gres], target=[something must be done], predicted=[something must be done]\n",
            "BLEU-1: 0.774012\n",
            "BLEU-2: 0.681110\n",
            "BLEU-3: 0.624264\n",
            "BLEU-4: 0.493274\n",
            "test\n",
            "src=[du kan ikke sige nej], target=[you cant say no], predicted=[you cant no no]\n",
            "src=[tom vil maske vidne], target=[tom might testify], predicted=[tom might testify]\n",
            "src=[thomas er fattig], target=[tom is poor], predicted=[tom is poor]\n",
            "src=[jeg er en smule sulten], target=[im a bit hungry], predicted=[im a bit hungry]\n",
            "src=[jeg har mistet min pung], target=[ive lost my wallet], predicted=[i lost my wallet]\n",
            "src=[hun var nedtrykt], target=[she felt blue], predicted=[she felt blue]\n",
            "src=[kan jeg hjlpe dig pa nogen made], target=[can i help you in any way], predicted=[can i ask you you you]\n",
            "src=[regner det nu], target=[is it raining now], predicted=[is it it now]\n",
            "src=[tom er endnu ikke teenager], target=[tom isnt a teenager yet], predicted=[tom isnt a teenager yet]\n",
            "src=[ls denne bog], target=[read this book], predicted=[is this book]\n",
            "BLEU-1: 0.739775\n",
            "BLEU-2: 0.642214\n",
            "BLEU-3: 0.587358\n",
            "BLEU-4: 0.457950\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22dDqbMLf9G3",
        "outputId": "ad04909a-64c3-4fca-efcf-615ed8025b3e"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None\n",
        "\n",
        "# generate target given source sequence\n",
        "def predict_sequence(model, tokenizer, source):\n",
        "\tprediction = model.predict(source, verbose=0)[0]\n",
        "\tintegers = [argmax(vector) for vector in prediction]\n",
        "\ttarget = list()\n",
        "\tfor i in integers:\n",
        "\t\tword = word_for_id(i, tokenizer)\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\ttarget.append(word)\n",
        "\treturn ' '.join(target)\n",
        "\n",
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
        "\tactual, predicted = list(), list()\n",
        "\tfor i, source in enumerate(sources):\n",
        "\t\t# translate encoded source text\n",
        "\t\tsource = source.reshape((1, source.shape[0]))\n",
        "\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n",
        "\t\traw_target, raw_src,test = raw_dataset[i]\n",
        "\t\tif i < 10:\n",
        "\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
        "\t\tactual.append([raw_target.split()])\n",
        "\t\tpredicted.append(translation.split())\n",
        "\t# calculate BLEU score\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-danish-both.pkl')\n",
        "train = load_clean_sentences('english-danish-train.pkl')\n",
        "test = load_clean_sentences('english-danish-test.pkl')\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "# prepare data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "\n",
        "# load model\n",
        "model = load_model('model.h5')\n",
        "# test on some training sequences\n",
        "print('train')\n",
        "evaluate_model(model, eng_tokenizer, trainX, train)\n",
        "# test on some test sequences\n",
        "print('test')\n",
        "evaluate_model(model, eng_tokenizer, testX, test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train\n",
            "src=[hvad lavede du om aftenen], target=[what did you do at night], predicted=[what did you do at night]\n",
            "src=[er det alt], target=[is that it], predicted=[is that it]\n",
            "src=[jeg har set en ufo], target=[i have seen a ufo], predicted=[i have seen a ufo]\n",
            "src=[hun var deprimeret], target=[she felt blue], predicted=[she felt blue]\n",
            "src=[jeg er bedre], target=[i am better], predicted=[im better better]\n",
            "src=[jeg tror tom kan lide mary], target=[i think tom likes mary], predicted=[i think tom likes mary]\n",
            "src=[jeg vil fortlle dig sandheden], target=[ill tell you the truth], predicted=[ill tell you the truth]\n",
            "src=[tom arbejder], target=[tom is working], predicted=[tom is]\n",
            "src=[aftensmaden er klar], target=[dinner is ready to eat], predicted=[dinners ready]\n",
            "src=[noget ma der gres], target=[something must be done], predicted=[something must be done]\n",
            "BLEU-1: 0.965638\n",
            "BLEU-2: 0.953126\n",
            "BLEU-3: 0.942366\n",
            "BLEU-4: 0.886245\n",
            "test\n",
            "src=[du kan ikke sige nej], target=[you cant say no], predicted=[you cant say no]\n",
            "src=[tom vil maske vidne], target=[tom might testify], predicted=[tom might testify]\n",
            "src=[thomas er fattig], target=[tom is poor], predicted=[tom is poor]\n",
            "src=[jeg er en smule sulten], target=[im a bit hungry], predicted=[im a bit hungry]\n",
            "src=[jeg har mistet min pung], target=[ive lost my wallet], predicted=[ive lost my wallet]\n",
            "src=[hun var nedtrykt], target=[she felt blue], predicted=[she felt blue]\n",
            "src=[kan jeg hjlpe dig pa nogen made], target=[can i help you in any way], predicted=[can i help you in any way]\n",
            "src=[regner det nu], target=[is it raining now], predicted=[is it raining now]\n",
            "src=[tom er endnu ikke teenager], target=[tom isnt a teenager yet], predicted=[tom isnt a teenager yet]\n",
            "src=[ls denne bog], target=[read this book], predicted=[read this book]\n",
            "BLEU-1: 0.918473\n",
            "BLEU-2: 0.895433\n",
            "BLEU-3: 0.884174\n",
            "BLEU-4: 0.821216\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}